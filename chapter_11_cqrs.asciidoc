[[chapter_11_cqrs]]
== Command-Query Responsibility Separation (CQRS)

In this chapter we're going to take a fairly uncontroversial insight--the
fact that reads (commands) and writes (queries) are different, and so they
should be treated differently--and we're going to push that insight out way
beyond the bounds of sanity and into fullblown boomtown architecture-astronaut
crazyland.  We may provide a few justifications along the way of when you
might or might not want to do it, bla-bla-bla-performance and so forth, but,
honestly, it's just because we want to see the world burn.  In a decoupled way.

NOTE: Chapter under construction.  More sober and coherent treatment of the subject
    may be found at
    https://io.made.com/blog/2017-09-13-commands-and-queries-handlers-and-views.html

// TODO: replace with cosmicpython.com url

So strap in, and see if you can spot the moment where the madness descends.
Because you need to be on the lookout for that moment in your own life.


=== Post/Redirect/Get

If you've been doing web development, you're probably familiar with the
post/redirect/get pattern. This is a technique where a web endpoint accepts an
HTTP POST, and responds with a redirect to see the result. For example, we might
accept a POST to `/batches` to create a new batch, and redirect the user to
`/batches/123` to see their newly created batch.

This approach fixes the problems that arise when users refresh the results page
in their browser, or try to bookmark a results page. In the case of a refresh,
it can lead to our users double-submitting data, buying two sofas where they
only needed one. In the case of a bookmark our hapless customers will end up
with a broken page when they try to GET a POST endpoint.

Both these problems happen because we're returning data in response to a write
operation. Post/Redirect/Get side-steps the issue by separating the read and
write phases of our operation.

This technique is a simple example of CQS - Command-Query Separation. In CQS we
follow one simple rule: functions should either modify state, or answer
questions, but never both. This makes software easier to reason about: we should
always be able to ask "are the lights on?" without flicking the light switch.

As we'll see, we can use the CQS principle to make our systems faster and more
scalable, but first, let's fix the CQS violation in our existing code. A few
chapters ago we introduced an `allocate` endpoint that takes an order and
calls our service layer to allocate some stock. At the end of the call, we
return a 200 OK and the batch id. That's led to some ugly design flaws so that
we can get the data we need. Let's change it to return a simple OK message, and
instead provide a new read-only endpoint to retrieve allocation state:


[[api_test_does_get_after_post]]
.API test does a GET after the POST (tests/e2e/test_api.py)
====
[source,python]
----
@pytest.mark.usefixtures('postgres_db')
@pytest.mark.usefixtures('restart_api')
def test_happy_path_returns_202_and_batch_is_allocated():
    orderid = random_orderid()
    sku, othersku = random_sku(), random_sku('other')
    batch1, batch2, batch3 = random_batchref(1), random_batchref(2), random_batchref(3)
    api_client.post_to_add_batch(batch1, sku, 100, '2011-01-02')
    api_client.post_to_add_batch(batch2, sku, 100, '2011-01-01')
    api_client.post_to_add_batch(batch3, othersku, 100, None)

    r = api_client.post_to_allocate(orderid, sku, qty=3)
    assert r.status_code == 202

    r = api_client.get_allocation(orderid)
    assert r.ok
    assert r.json() == [
        {'sku': sku, 'batchref': batch2},
    ]


@pytest.mark.usefixtures('postgres_db')
@pytest.mark.usefixtures('restart_api')
def test_unhappy_path_returns_400_and_error_message():
    unknown_sku, orderid = random_sku(), random_orderid()
    r = api_client.post_to_allocate(
        orderid, unknown_sku, qty=20, expect_success=False,
    )
    assert r.status_code == 400
    assert r.json()['message'] == f'Invalid sku {unknown_sku}'

    r = api_client.get_allocation(orderid)
    assert r.status_code == 404
----
====


OK what might the flask app look like?


[[flask_app_calls_view]]
.Endpoint for viewing allocations (src/allocation/flask_app.py)
====
[source,python]
----
from allocation import commands, exceptions, messagebus, orm, unit_of_work, views
...

@app.route("/allocations/<orderid>", methods=['GET'])
def allocations_view_endpoint(orderid):
    uow = unit_of_work.SqlAlchemyUnitOfWork()
    result = views.allocations(orderid, uow)  #<1>
    if not result:
        return 'not found', 404
    return jsonify(result), 200
----
====

<1> All right, a _views.py_, fair enough, we can keep read-only stuff in there,
    and it'll be a real views.py, not like Django's, something that knows how
    to build read-only views of our data...


=== Hold on to Your Lunch Folks.

...so we can probably just add a list method to our existing repository
obj-...


[[views_dot_py]]
.Views do... raw sql??? (src/allocation/views.py)
====
[source,python]
[role="non-head"]
----
from allocation import unit_of_work

def allocations(orderid: str, uow: unit_of_work.SqlAlchemyUnitOfWork):
    with uow:
        results = list(uow.session.execute(
            'SELECT ol.sku, b.reference'
            ' FROM allocations AS a'
            ' JOIN batches AS b ON a.batch_id = b.id'
            ' JOIN order_lines AS ol ON a.orderline_id = ol.id'
            ' WHERE ol.orderid = :orderid',
            dict(orderid=orderid)
        ))
    return [{'sku': sku, 'batchref': batchref} for sku, batchref in results]
----
====

WHAT THE ACTUAL F?  ARE YOU GUYS TRIPPING F-ING BALLS?

Yes.  Yes we are.  Obviously don't do this.  Unless you have Reasons (TM). Let's come
back to that and assume you don't.  You probably don't.  Go ahead and do
something sane instead:


[[view_using_repo]]
.A simple view that uses the repository (src/allocation/views.py)
====
[source,python]
[role="skip"]
----
from allocation import unit_of_work

def allocations(orderid: str, uow: unit_of_work.AbstractUnitOfWork):
    with uow:
        products = uow.products.for_order(orderid=orderid)
        batches = [b for p in products for b in p.batches]
        return [
            {'sku': b.sku, 'batchref': b.reference}
            for b in batches
            if orderid in b.orderids
        ]
----
====

We're sure you can figure out how to write a query called `.for_order()`
on your repository that can find all the products for an order.


Note that splitting out a _views.py_ module and enforcing a clear
distinction between reads and writes in your application is still
probably a good idea.  We apply command-query separation, and it's easy to see
which code modifies state (the event handlers) and which code just retrieves
read-only state (the views).

TIP: Split out your read-only views from your state-modifying
    command and event handlers.


=== Why on Earth Would You Write Raw SQL in Your Views?

OK Bob and Harry, let's get back to that SQL monstrosity though.  Why
on earth would you show us that?  Don't think we're about to forget, the
sheer horror has been permanently burned into our memories.  Why on
earth would you want to do that? _Ever?_

Broadly speaking there are two reasons:

1. Performance.
2. Keeping your Domain Model free of Spurious Concerns.


==== SELECT N+1

The so-called
https://secure.phabricator.com/book/phabcontrib/article/n_plus_one/[SELECT N+1]
problem is a common performance problem with ORMs: when retrieving a list of
objects, your ORM will often perform an initial query to, say, get all the IDs
of the objects it needs, and then issue individual queries for each object to
retrieve their attributes.  This is especially likely if there are any foreign
key relationships on your objects.

//TODO: set echo=True and show SQL query logs in our own app?


==== General Database Read Performance

Beyond `SELECT N+1`, you may have other reasons that you want to decouple the
way you persist state changes from the way that you retrieve current state.
A set of fully normalized relational tables is a good way to make sure that
write operations never cause data corruption.  But retrieving data using lots
of JOINs can be slow.  It's common in such cases to add some denormalized views
(we'll see an example of that later), build read replicas, or even add caching
layers.


==== Your Domain Model is not Optimized for Read Operations

Now here's the chinstrokey-architect justification.  As we've said before,
a Domain Model is not a data model--we're trying to capture the way the
business works: workflow, rules around state changes, messages exchanged;
concerns about how the system reacts to external events and user input.
_Most of this stuff is totally irrelevant for read-only operations_.

Making a facile point, your domain classes will have a number of methods for
modifying state, and you won't need any of them for read-only operations.
Now that is very, very, very far away from a justification for using raw
SQL instead of reusing our domain objects for queries, but you can at
least see how the two are conceptually different.

As the complexity of your domain model grows though, you may find yourself
making more and more choices about how to structure that model, which make
it more and more awkward to use for read operations.

Even in our simple example, we've chosen to use `Product` as our aggregate,
but for our "show me the allocations for this order id" endpoint, going
via Product isn't really the most obvious way of doing things.  In
<<view_using_repo>> we retrieve all the products for the skus in the order,
then we find all the batches for those products, and _then_ we iterate (slowly,
in Python) through all of them finding the ones that have allocations for that
order id?  It's clunky.  And we didn't mention it at the time, but we had to
add a new `@property` to the domain model to be able to get the order ids
allocated to a batch:

[[orderids_on_batch]]
.An argubably-uneccessary property on our model (src/allocation/model.py)
====
[source,python]
[role="skip"]
----
class Batch:
    ...

    @property
    def orderids(self):
        return {l.orderid for l in self._allocations}
----
====


=== Using the ORM is Probably Simpler.  Probably.

You may be thinking, OK, if our repository is clunky, then I can just use my
ORM.  That's what it's for!

[[view_using_orm]]
.A simple view that uses the ORM (src/allocation/views.py)
====
[source,python]
[role="skip"]
----
from allocation import unit_of_work, model

def allocations(orderid: str, uow: unit_of_work.AbstractUnitOfWork):
    with uow:
        batches = uow.session.query(model.Batch).join(
            model.OrderLine, model.Batch._allocations
        ).filter(
            model.OrderLine.orderid == orderid
        )
        return [
            {'sku': b.sku, 'batchref': b.batchref}
            for b in batches
        ]
----
====

But is that _actually_ any easier to write or understand than the raw SQL
version from <<views_dot_py>>?  It may not look too bad up there, but we
can tell you it took several attempts, and plenty of digging through the
SQLAlchemy docs.  SQL is just SQL.


=== Testing CQRS Views

Let's talk about testing.  Whichever of the approaches you decide to go for,
the most obvious kind of test to write is an integration test, one that goes
to a real database.

This test would work for any of the approaches we've shown so far:


[[integration_testing_views]]
.An integration test for a view (tests/integration/test_views.py)
====
[source,python]
----
from datetime import date
from allocation import commands, unit_of_work, messagebus, views


def test_allocations_view(sqlite_session_factory):
    uow = unit_of_work.SqlAlchemyUnitOfWork(sqlite_session_factory)
    messagebus.handle(commands.CreateBatch('sku1batch', 'sku1', 50, None), uow)
    messagebus.handle(commands.CreateBatch('sku2batch', 'sku2', 50, date.today()), uow)
    messagebus.handle(commands.Allocate('order1', 'sku1', 20), uow)
    messagebus.handle(commands.Allocate('order1', 'sku2', 20), uow)
    # add a spurious batch and order to make sure we're getting the right ones
    messagebus.handle(commands.CreateBatch('sku1batch-later', 'sku1', 50, date.today()), uow)
    messagebus.handle(commands.Allocate('otherorder', 'sku1', 30), uow)
    messagebus.handle(commands.Allocate('otherorder', 'sku2', 10), uow)

    assert views.allocations('order1', uow) == [
        {'sku': 'sku1', 'batchref': 'sku1batch'},
        {'sku': 'sku2', 'batchref': 'sku2batch'},
    ]
----
====

Before you dismiss the need to use integration tests as just another
anti-feather in the anti-cap of this total anti-pattern, it's worth thinking
through the alternatives.

- If you're going via the `Products` repository, then you'll need integration
    tests for the `.for_order()` helper method

- If you're going via the ORM, you'll still need integration tests

- And if you decide to build a read-only `BatchRepository`, ignoring
  the purists that tell you you're not allowed to have a Repository for
  a non-Aggregate model class, call it `BatchDAL` if you want, in any case,
  you'll still need integration tests for _that_.

So the choice is about whether or not you want a layer of abstraction between
your permanent storage and the logic of your read-only views.

* If the views are relatively simple (all the logic in our case is in filtering
  down to the right batch references), then adding another layer doesn't seem
  worth it.

* If your views do more complex calculations, or need to invoke some business rules
  to decide what to display... If, in short, you find yourself writing a lot of
  integration tests for a single view, then it may be worth building that
  intermediary layer, so that you can test the SQL and the display/calculation/view
  logic separately


// TODO: some example code showing a DAL layer in front of some read-only view
// code with more complex business logic.


=== Doubling Down on the Madness.

Have we convinced you that our raw SQL version isn't so crazy as it first
seemed?  Perhaps we were exaggerating the craziness for effect?

Just you wait.

So. Crazy or not, that hardcoded SQL query is pretty ugly right?  What if we
made it nicer by keeping a totally separate, denormalized datastore for our
view model?

Horrifying, right?  Wait 'til we tell you we're not even going to use Postgres
views, or triggers, or anything known and reliable and boring like that, to
keep it up to date.  We're going to use our amazing event-driven architecture!
That's right!  May as well join the cult and start drinking folks, the ship is
made of cardboard and the captains are crazy and there's nothing you can do to
stop them.


[[much_nicer_query]]
.A much nicer query (src/allocation/views.py)
====
[source,python]
----
def allocations(orderid: str, uow: unit_of_work.SqlAlchemyUnitOfWork):
    with uow:
        results = list(uow.session.execute(
            'SELECT sku, batchref FROM allocations_view WHERE orderid = :orderid',
            dict(orderid=orderid)
        ))
        ...
----
====

Here's our table.  Hee hee hee, no foreign keys, just strings, YOLO.

[[new_table]]
.A very simple table (src/allocation/orm.py)
====
[source,python]
----
allocations_view = Table(
    'allocations_view', metadata,
    Column('orderid', String(255)),
    Column('sku', String(255)),
    Column('batchref', String(255)),
)
----
====

We add a second handler to the `Allocated` event:

[[new_handler_for_allocated]]
.Allocated event gets a new handler (src/allocation/messagebus.py)
====
[source,python]
----
EVENT_HANDLERS = {
    events.Allocated: [
        handlers.publish_allocated_event,
        handlers.add_allocation_to_read_model
    ],
----
====


Here's what our update-view-model code looks like:


[[update_view_model_1]]
.Update on allocation (src/allocation/handlers.py)
====
[source,python]
----

def add_allocation_to_read_model(
        event: events.Allocated, uow: unit_of_work.SqlAlchemyUnitOfWork,
):
    with uow:
        uow.session.execute(
            'INSERT INTO allocations_view (orderid, sku, batchref)'
            ' VALUES (:orderid, :sku, :batchref)',
            dict(orderid=event.orderid, sku=event.sku, batchref=event.batchref)
        )
        uow.commit()
----
====


And it'll work!


(OK you'll also need to handle deallocated:)


[[handle_deallocated_too]]
.A second listener for read model updates
====
[source,python]
[role="skip"]
----
events.Deallocated: [
    handlers.remove_allocation_from_read_model,
    handlers.reallocate
],

...

def remove_allocation_from_read_model(
        event: events.Deallocated, uow: unit_of_work.SqlAlchemyUnitOfWork,
):
    with uow:
        uow.session.execute(
            'DELETE FROM allocations_view '
            ' WHERE orderid = :orderid AND sku = :sku',
----
====


<<read_model_sequence_diagram>> shows the flow across the two requests: two
transactions in the POST/write operation, one to update the write model and one
to update the read model, which the GET/read operation can use.

[[read_model_sequence_diagram]]
.Sequence diagram for read model
image::images/read_model_sequence_diagram.png[]
[role="image-source"]
----
[plantuml, read_model_sequence_diagram, config=plantuml.cfg]
@startuml
actor User order 1
boundary Flask order 2
participant MessageBus order 3
participant "Domain Model" as Domain order 4
participant View order 9
database DB order 10

User -> Flask: POST to allocate Endpoint
Flask -> MessageBus : Allocate Command

group UoW/transaction 1
    MessageBus -> Domain : allocate()
    MessageBus -> DB: commit write model
end

group UoW/transaction 2
    Domain -> MessageBus : raise Allocated event(s)
    MessageBus -> DB : update view model
end

Flask -> User: 202 OK

User -> Flask: GET allocations endpoint
Flask -> View: get allocations
View -> DB: SELECT on view model
DB -> View: some allocations
View -> Flask: some allocations
Flask -> User: some allocations

@enduml
----


=== But Whyyyyyyy?

OK.  Horrible, right?  But also, kinda, surprisingly nice, considering?  Our
events and message bus give us a really nice place to do this sort of stuff,
_if we need to_.

And think how easy it'd be to swap our read model from Postgres to Redis or
Memcached? Super-simple.  _We don't even need to change the integration test_.


=== Changing our Read Model Implementation is Easy

Just watch.


[[redis_readmodel_handlers]]
.Handlers update a redis read model (src/allocation/handlers.py)
====
[source,python]
[role="non-head"]
----
def add_allocation_to_read_model(event: events.Allocated, _):
    redis_pubsub.update_readmodel(event.orderid, event.sku, event.batchref)

def remove_allocation_from_read_model(event: events.Deallocated, _):
    redis_pubsub.update_readmodel(event.orderid, event.sku, None)
----
====

The helpers in our redis module are one-liners:


[[redis_readmodel_client]]
.Redis read model read + update (src/allocation/redis_pubsub.py)
====
[source,python]
[role="non-head"]
----
def update_readmodel(orderid, sku, batchref):
    r.hset(orderid, sku, batchref)


def get_readmodel(orderid):
    return r.hgetall(orderid)
----
====

And the view itself changes very slightly to adapt to its new backend:

[[redis_readmodel_view]]
.View adapted to redis (src/allocation/views.py)
====
[source,python]
[role="non-head"]
----
def allocations(orderid):
    batches = redis_pubsub.get_readmodel(orderid)
    return [
        {'batchref': b.decode(), 'sku': s.decode()}
        for s, b in batches.items()
    ]
----
====

And the _exact same_ integration tests that we had before still pass,
because they are written at a level of abstraction that's decoupled from the
implementation: setup puts messages on the messagebus, and the assertions
are against our view.

TIP: Event handlers are a great way to manage updates to a read model,
    if you decide you need one.  They also make it easy to change the
    implementation of that read model at a later date.



=== But Would You Really?  CRUD versus CQRS.

// TODO (EJ) Some explicit discussion of CRUD vs CQRS might be good. I'd guess
// that there will be a lot readers coming to this book from Django.

As it happens, the allocation service at MADE.com does use "full blown" CQRS,
with a read model that uses Redis, and even a second layer of cache provided
by Varnish.  But its use cases are actually quite a bit different from what
we've shown here. For the kind of allocation service we're building, it seems
unlikely that you'd need to use a separate read model and event handlers for
updating it.

But once you commit to using a Domain Model rather than "just" building a
CRUD app, then some level of CQS or CQRS does become more and more necessary.

// TODO have we really explained the difference between CQS and CQRS?  maybe
// there isn't one really so we shouldn't use them this way?

Often, your read operations will acting on the same conceptual objects as your
write model, so using the ORM, adding some read methods to your repositories,
and using Domain Model classes for your read operations is _just fine_. 

As it happens in our case, our read operations act on quite different
conceptual entities to our Domain Model.  The allocation service thinks
in terms of `Batches` for a single sku, but users care about allocations
for a whole order, with multiple skus, so using the ORM ends up being a little
awkward.  We'd be quite tempted to go with the raw-SQL view we showed right
at the beginning of the chapter.


OK.  On that note, let's sally forth into our final chapter.
