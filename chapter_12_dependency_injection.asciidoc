[[chapter_12_dependency_injection]]
== Dependency Injection (And Mocks)

//TODO get rid of bullets

.In this chapter
********************************************************************************

* We'll show how dependency injection supports our architectural goals.
* We'll introduce a _composition root_ pattern to bootstrap our system.
* We'll offer some guidance on managing application configuration.
* We'll compare different approaches to dependency injection and discuss their
  trade-offs.

  // DIAGRAM GOES HERE

********************************************************************************

NOTE: chapter under construction

Depending on your particular brain type, you may have a slight feeling of
unease at the back of your mind at this point.  Let's bring it out into the
open. We've currently shown two different ways of managing dependencies, and
testing them.

[TIP]
====
You can find our code for this chapter at
https://github.com/cosmicpython/code/tree/chapter_12_dependency_injection[github.com/cosmicpython/code/tree/chapter_12_dependency_injection].

----
git clone https://github.com/cosmicpython/code.git && cd code
git checkout chapter_12_dependency_injection
# or, if you want to code along, checkout the previous chapter.
git checkout chapter_11_cqrs
----
====

=== Implicit vs Explicit Dependencies

TIP: If you haven't already, it's worth reading <<chapter_03_abstractions>>
    before continuing with this chapter, particularly the discussion of
    FCIS vs DI.


For our database dependency, we've built a careful framework of explicit
dependencies and easy options for overriding them in tests. Our main handler
functions declare an explicit dependency on the unit of work:

[[existing_handler]]
.Our handlers have an explicit dependency on the UoW (src/allocation/service_layer/handlers.py)
====
[source,python]
[role="existing"]
----
def allocate(
        cmd: commands.Allocate, uow: unit_of_work.AbstractUnitOfWork
):
----
====

And that makes it easy to swap in a fake unit of work in our
service-layer tests

[[existing_services_test]]
.Service layer tests against a fake uow: (tests/unit/test_services.py)
====
[source,python]
[role="skip"]
----
    uow = FakeUnitOfWork()
    messagebus.handle([...], uow)
----
====


The UoW itself declares an explicit dependency on the session factory:


[[existing_uow]]
.The UoW depends on a session factory (src/allocation/service_layer/unit_of_work.py)
====
[source,python]
[role="existing"]
----
class SqlAlchemyUnitOfWork(AbstractUnitOfWork):

    def __init__(self, session_factory=DEFAULT_SESSION_FACTORY):
        self.session_factory = session_factory
        ...
----
====

We take advantage of it in our integration tests to be able to use sqlite
instead of Postgres, sometimes

[[existing_integration_test]]
.Integration tests against a different DB (tests/integration/test_uow.py)
====
[source,python]
[role="existing"]
----
def test_rolls_back_uncommitted_work_by_default(sqlite_session_factory):
    uow = unit_of_work.SqlAlchemyUnitOfWork(sqlite_session_factory)  #<1>
----
====

<1> Integration tests swap out the default postgres session_factory for a
    sqlite one.




=== Explicit Dependencies Are Totally Weird an Java-Ey Tho

If you're used to the way things normally happen in Python, you'll be thinking
all this is a bit weird.  The standard way to do things is to declare our
dependency "implicitly" by simply importing it, and then if we ever need to
change it for tests, we can monkeypatch, as is Right and True in dynamic
languages:


[[normal_implicit_dependency]]
.Email-sending as a normal import-based dependency (src/allocation/service_layer/handlers.py)
====
[source,python]
[role="existing"]
----
from allocation.adapters import email, redis_eventpublisher  #<1>
...

def send_out_of_stock_notification(
        event: events.OutOfStock, uow: unit_of_work.AbstractUnitOfWork,
):
    email.send(  #<2>
        'stock@made.com',
        f'Out of stock for {event.sku}',
    )
----
====

<1> hardcoded import
<2> calls specific email sender directly.


Why pollute our application code with unnecessary arguments just for the
sake of our tests? `mock.patch` makes monkeypatching nice and easy:


[[mocking_is_easy]]
.mock dot patch, thank you Michael Foord (tests/unit/test_handlers.py)
====
[source,python]
[role="existing"]
----
    with mock.patch("allocation.adapters.email.send") as mock_send_mail:
        ...
----
====

The trouble is that we've made it look easy because our toy example doesn't
send real emails (`email.send_mail` just does a `print`), but in real life
you'd end up having to call `mock.patch` for _every single test_ that might
cause an out-of-stock notification. If you've worked on codebases with lots of
mocks used to prevent unwanted side-effects, you'll know how annoying that
mocky boilerplate gets.

And, you'll know that mocks tightly couple us to the implementation.  By
choosing to monkeypatch `email.send_mail`, we are tied to doing `import email`,
and if we ever want to do `from email import send_mail`, a trivial refactor,
we'd have to change all our mocks.

So it's a trade-off.  Yes declaring explicit dependencies is "unnecessary,"
strictly speaking, and using them would make our application code marginally
more complex.  But in return, we'd get tests that are easier to write and
manage.

On top of which, declaring an explicit dependency is an implementation of
the DIP--rather than having an (implicit) dependency on a _specific_ detail,
we have an (explicit) dependency on an _abstraction_:

// (DS) great section

[quote, The Zen of Python]
____
Explicit Is Better Than Implicit
____


[[handler_with_explicit_dependency]]
.The explicit dependency is more abstract (src/allocation/service_layer/handlers.py)
====
[source,python]
[role="non-head"]
----
def send_out_of_stock_notification(
        event: events.OutOfStock, send_mail: Callable,
):
    send_mail(
        'stock@made.com',
        f'Out of stock for {event.sku}',
    )
----
====


But if we do change to declaring all these dependencies explicitly, who will
inject them and how? So far, we've only really been dealing with passing the
UoW around:  our tests use a `FakeUnitOfWork` while Flask and Redis eventconsumer
entrypoints use the real UoW, and the messagebus passes them onto our command
handlers.  If we add real and fake email classes, who will create them and
pass them on?

It's extra (duplicated) cruft for Flask, Redis and our tests.  Moreover,
putting all the responsibility for passing dependencies to the right handler
onto the messagebus feels like a violation of the SRP.

Instead, we'll reach for a pattern called _Composition Root_ (a bootstrap
script to you and me), and we'll do a bit of "Poor Man's DI" (dependency
injection without a framework).

----
Flask + Redis
|
| call
V
Bootstraper:  prepares handlers with correct dependencies injected in
|             (test bootstrapper will use fakes, prod one will use reals)
|
|  pass injected handlers to
V
Messagebus
|
|  dispatches events and commands to injected handlers
|
V
----

TODO: betterify diagram




=== Preparing Handlers: Poor Man's DI with Partial Functions

One way to turn a function with dependencies into one that's ready to be
called later with those dependencies _aleady injected_, is to use partial
functions:


[[di_with_partial_functions_examples]]
.Examples of DI using partial functions
====
[source,python]
[role="skip"]
----
# existing allocate function, with abstract uow dependency
def allocate(
        cmd: commands.Allocate, uow: unit_of_work.AbstractUnitOfWork
):
    line = OrderLine(cmd.orderid, cmd.sku, cmd.qty)
    with uow:
        ...

# bootstrap script prepares actual UoW

uow = unit_of_work.SqlAlchemyUnitOfWork()

# then prepares a version of the allocate fn with dependencies already injected
allocate_partial = lambda cmd: allocate(cmd, uow)

#  or, alternatively
import functools
allocate_partial = functools.partial(allocate, uow=uow)

...
# later at runtime, we can call the partial function, and it will have
# the UoW already injected
allocate_partial(cmd)
----
====


Here's the same pattern again for the `send_out_of_stock_notification()` handler,
which has different dependencies:

[[partial_functions_2]]
.Another partial functions example
====
[source,python]
[role="skip"]
----
def send_out_of_stock_notification(
        event: events.OutOfStock, send_mail: Callable,
):
    send_mail(
        'stock@made.com',
        ...


# prepare a version of the send_out_of_stock_notification with dependencies
sosn_partial = lambda event: send_out_of_stock_notification(event, email.send_mail)

...
# later, at runtime:
sosn_partial(event)  # will have email.send_mail already injected in
----
====


=== An alternative using classes

Partial functions will feel familiar to people who've done a bit of functional
programming.   Here's an alternative using classes, which may appeal to other
people.   It requires rewriting all our handler functions as classes though:

[[di_with_classes]]
.DI using classes
====
[source,python]
[role="skip"]
----
# we replace the old `def allocate(cmd, uow)` with:

class AllocateHandler:

    def __init__(self, uow: unit_of_work.AbstractUnitOfWork):  #<2>
        self.uow = uow

    def __call__(self, cmd: commands.Allocate):  #<1>
        line = OrderLine(cmd.orderid, cmd.sku, cmd.qty)
        with self.uow:
            # rest of handler method as before
            ...

# bootstrap script prepares actual UoW
uow = unit_of_work.SqlAlchemyUnitOfWork()

# then prepares a version of the allocate fn with dependencies already injected
allocate = AllocateHandler(uow)

...
# later at runtime, we can call the handler instance, and it will have
# the UoW already injected
allocate(cmd)
----
====

<1> The class is designed to produce a callable function, so it has a
    ++__call__++ method.

<2> But we use the ++__init__++ to declare the dependencies it requires.
    This sort of thing will feel familiar if you've ever made class-based
    descriptors, or a class-based context manager that takes arguments.


Use whichever you and your team feel more comfortable with.


=== Bootstrap script does all the DI

Our bootstrapper will do any required application initialization,
then prepare some dependency-injected handlers, give them to our
message bus, and hand that back to us.


//TODO: show a version without inspect()

[[bootstrap_v1]]
.A bootstrap function (src/allocation/bootstrap.py)
====
[source,python]
----
def bootstrap(
    start_orm: bool = True,  #<1>
    uow: unit_of_work.AbstractUnitOfWork = unit_of_work.SqlAlchemyUnitOfWork(),
    send_mail: Callable = email.send,
    publish: Callable = redis_eventpublisher.publish,
) -> messagebus.MessageBus:

    if start_orm:
        orm.start_mappers()  #<1>

    dependencies = {'uow': uow, 'send_mail': send_mail, 'publish': publish}
    injected_event_handlers = {  #<2>
        event_type: [
            inject_dependencies(handler, dependencies)
            for handler in event_handlers
        ]
        for event_type, event_handlers in handlers.EVENT_HANDLERS.items()
    }
    injected_command_handlers = {  #<2>
        command_type: inject_dependencies(handler, dependencies)
        for command_type, handler in handlers.COMMAND_HANDLERS.items()
    }

    return messagebus.MessageBus(  #<3>
        uow=uow,
        event_handlers=injected_event_handlers,
        command_handlers=injected_command_handlers,
    )
----
====

<1>  `orm.start_mappers()` is our example of initialization work that needs
    to be done once at the beginning of an app.  We also see things like
    setting up the `logging` module, TODO more examples

<2> We build up our injected versions of the handlers mappings using
    a function called `inject_dependencies()` which we'll show next.

<5> And we return a configured message bus ready to use.


Here's how we inject dependencies into a handler function by inspecting
it:

[[di_by_inspection]]
.DI by inspecting function signatures (src/allocation/bootstrap.py)
====
[source,python]
----
def inject_dependencies(handler, dependencies):
    params = inspect.signature(handler).parameters  #<1>
    deps = {
        name: dependency
        for name, dependency in dependencies.items()  #<2>
        if name in params
    }
    return lambda message: handler(message, **deps)  #<3>
----
====

<1> We inspect our command/event handler's arguments
<2> We match them by name to our dependencies
<3> And we inject them in as kwargs to a produce a partial



=== Messagebus Gets Given Handlers at Runtime

Our messagebus will no longer be static, it needs to have the already-injected
handlers given to it.  So we turn it from being a module into a configurable
class:


[[messagebus_as_class]]
.MessageBus as a class (src/allocation/service_layer/messagebus.py)
====
[source,python]
[role="non-head"]
----
class MessageBus:  #<1>

    def __init__(
        self,
        uow: unit_of_work.AbstractUnitOfWork,
        event_handlers: Dict[Type[events.Event], List[Callable]],  #<2>
        command_handlers: Dict[Type[commands.Command], Callable],  #<2>
    ):
        self.uow = uow
        self.event_handlers = event_handlers
        self.command_handlers = command_handlers

    def handle(self, message: Message):  #<3>
        self.queue = [message]
        while self.queue:
            message = self.queue.pop(0)
            if isinstance(message, events.Event):
                self.handle_event(message)
            elif isinstance(message, commands.Command):
                self.handle_command(message)
            else:
                raise Exception(f'{message} was not an Event or Command')
----
====

<1> The messagebus becomes a class...
<2> ...which is given its already-dependency-injected handlers
<3> the main `handle()` function is substantially the same, just
    moving a few methods onto `self`.

// TODO (DS) diff?


What else changes in the bus? 

[[messagebus_handlers_change]]
.Event and Command handler logic stays the same (src/allocation/service_layer/messagebus.py)
====
[source,python]
----
    def handle_event(self, event: events.Event):
        for handler in self.event_handlers[type(event)]:  #<1>
            try:
                logger.debug('handling event %s with handler %s', event, handler)
                handler(event)  #<2>
                self.queue.extend(self.uow.collect_new_events())
            except:
                logger.exception('Exception handling event %s', event)
                continue


    def handle_command(self, command: commands.Command):
        logger.debug('handling command %s', command)
        try:
            handler = self.command_handlers[type(command)]  #<1>
            handler(command)  #<2>
            self.queue.extend(self.uow.collect_new_events())
        except Exception:
            logger.exception('Exception handling command %s', command)
            raise
----
====

<1> `handle_event` and `handle_command` are substantially the same, but instead
    of indexing into a static `EVENT_HANDLERS` or `COMMAND_HANDLERS` dict, they
    use the versions on `self`.

<2> and instead of passing a UoW into the handler, they expect the handlers
    to already have all their dependencies, so all they need is a single argument,
    the specific event or command.


=== Using Bootstrap in Our Entrypoints

In our application's entrypoints, we now just call `bootstrap.bootstrap()`
and get a messagebus that's ready to go, rather than configuring a UoW and the
rest of it.

[[flask_calls_bootstrap]]
.Flask calls bootstrap (src/allocation/entrypoints/flask_app.py)
====
[source,diff]
----
-from allocation import views
+from allocation import bootstrap, views
 
 app = Flask(__name__)
-orm.start_mappers()
+bus = bootstrap.bootstrap()
 
 
 @app.route("/add_batch", methods=['POST'])
@@ -19,8 +16,7 @@ def add_batch():
     cmd = commands.CreateBatch(
         request.json['ref'], request.json['sku'], request.json['qty'], eta,
     )
-    uow = unit_of_work.SqlAlchemyUnitOfWork()
-    messagebus.handle(cmd, uow)
+    bus.handle(cmd)
     return 'OK', 201

----
====

// TODO (DS): starting to make more sense once we get to here.  Could you start
// with the entry point and then show the implementation after?


// (ej) The "Flask-onic" version of bootstrap is the application factory pattern w/ blueprints.
//
// The issue that the style above (where app is a module variable) can raise, is that the import will
// have side-effects, because it inits the bus.  In the worst case, someone may have decided to add
// network calls to retrieve config.
//
// If you end up needing to unit test somethihng in this module, or use the flask test client, 
// it can then lead you down the path to patching the configuration, which defeats the composition root.
// 
// I think Docker helps mitigate this problem, with the cost of potentially increasing the complexity.
//
//  Reference:
//  https://flask.palletsprojects.com/en/1.1.x/patterns/appfactories/#
//  https://flask.palletsprojects.com/en/1.1.x/testing/#other-testing-tricks
//  https://flask.palletsprojects.com/en/1.1.x/blueprints/#blueprints


=== Initializing DI in Our Tests


And in tests, we can use our `bootstrap.bootstrap()` with overridden defaults
to get a custom messagebus:


[[custom_bootstrap]]
.Overriding bootstrap defaults (tests/integration/test_views.py)
====
[source,python]
----
@pytest.fixture
def sqlite_bus(sqlite_session_factory):
    bus = bootstrap.bootstrap(
        start_orm=True,
        uow=unit_of_work.SqlAlchemyUnitOfWork(sqlite_session_factory),
        send_mail=lambda *args: None,
        publish=lambda *args: None,
    )
    yield bus
    clear_mappers()

def test_allocations_view(sqlite_bus):
    sqlite_bus.handle(commands.CreateBatch('sku1batch', 'sku1', 50, None))
    sqlite_bus.handle(commands.CreateBatch('sku2batch', 'sku2', 50, date.today()))
    ...
    assert views.allocations('order1', sqlite_bus.uow) == [
        {'sku': 'sku1', 'batchref': 'sku1batch'},
        {'sku': 'sku2', 'batchref': 'sku2batch'},
    ]
----
====


// TODO: show option of bootstrapper as class instead?


=== Building an Adapter "Properly": A Worked Example

We've got two types of dependency:

[[two_types_of_dependency]]
.Two types of dependency (src/allocation/service_layer/messagebus.py)
====
[source,python]
[role="skip"]
----
            uow: unit_of_work.AbstractUnitOfWork,  #<1>
            send_mail: Callable,  #<2>
            publish: Callable,  #<2>
----
====

<1> the UoW has an abstract base class.  This is the heavyweight
    option for declaring and managing your external dependency.
    We'd use this for case when the dependency is relatively complex

<2> our email sender and pubsub publisher are just defined
    as functions.  This works just fine for simple things.

Here are some of the things we find ourselves injecting at work:

* an S3 filesystem client
* a key/value store client
* a `requests` session object.

Most of these will have more complex APIs that you can't capture
as a single function.  Read and write, GET and POST, and so on.

Even though it's simple, let's use `send_mail` as an example to talk
through how you might define a more complex dependency.


==== Define the Abstract and Concrete Implementations

We'll imagine a more generic "notifications" API.  Could be
email, could be SMS, could be slack posts one day.


[[notifications_dot_py]]
.An ABC and a concrete implementation (src/allocation/adapters/notifications.py)
====
[source,python]
----
class AbstractNotifications(abc.ABC):

    @abc.abstractmethod
    def send(self, destination, message):
        raise NotImplementedError

...

class EmailNotifications(AbstractNotifications):

    def __init__(self, smtp_host=DEFAULT_HOST, port=DEFAULT_PORT):
        self.server = smtplib.SMTP(smtp_host, port=port)
        self.server.noop()

    def send(self, destination, message):
        msg = f'Subject: allocation service notification\n{message}'
        self.server.sendmail(
            from_addr='allocations@example.com',
            to_addrs=[destination],
            msg=msg
        )
----
====


we change the dependency in the messagebus:

[[notifications_in_bus]]
.Notifications in messagebus (src/allocation/service_layer/messagebus.py)
====
[source,python]
----
class MessageBus:

    def __init__(
            self,
            uow: unit_of_work.AbstractUnitOfWork,
            notifications: notifications.AbstractNotifications,
            publish: Callable,
    ):
----
====



We work through and define a fake version for unit testing:


[[fake_notifications]]
.fake notifications (tests/unit/fakes.py)
====
[source,python]
----
class FakeNotifications(notifications.AbstractNotifications):

    def __init__(self):
        self.sent = defaultdict(list)  # type: Dict[str, List[str]]

    def send(self, destination, message):
        self.sent[destination].append(message)

...

class FakeBus(messagebus.MessageBus):
    def __init__(self):
        uow = FakeUnitOfWork()
        super().__init__(
            uow=uow,
            notifications=FakeNotifications(),
            publish=mock.Mock(),
        )
        uow.bus = self
----
====

we can use it in our tests:

[[test_with_fake_notifs]]
.Tests change slightly (tests/unit/test_handlers.py)
====
[source,python]
----
    def test_sends_email_on_out_of_stock_error():
        bus = FakeBus()
        bus.handle(commands.CreateBatch("b1", "POPULAR-CURTAINS", 9, None))
        bus.handle(commands.Allocate("o1", "POPULAR-CURTAINS", 10))
        assert bus.dependencies['notifications'].sent['stock@made.com'] == [
            f"Out of stock for POPULAR-CURTAINS",
        ]
----
====


Now we test the real thing, usually with an end-to-end or integration
test.  We've used https://github.com/mailhog/MailHog[MailHog] as a
real-ish email server for our docker dev environment.


[[docker_compose_with_mailhog]]
.Docker-compose config with real fake email server (docker-compose.yml)
====
[source,yaml]
----
version: "3"

services:

  redis_pubsub:
    build:
      context: .
      dockerfile: Dockerfile
    image: allocation-image
    ...

  api:
    image: allocation-image
    ...

  postgres:
    image: postgres:9.6
    ...

  redis:
    image: redis:alpine
    ...

  mailhog:
    image: mailhog/mailhog
    ports:
      - "11025:1025"
      - "18025:8025"
----
====


In our integration tests, we use the real `EmailNotifications` class,
talking to the MailHog server in the docker cluster:



[[integration_test_email]]
.Integration test for email (tests/integration/test_email.py)
====
[source,python]
----
cfg = config.get_email_host_and_port()

@pytest.fixture
def bus(sqlite_session_factory):
    uow = unit_of_work.SqlAlchemyUnitOfWork(sqlite_session_factory)
    bus = messagebus.MessageBus(
        uow=uow,
        notifications=notifications.EmailNotifications(
            smtp_host=cfg['host'],
            port=cfg['port'],
        ),
        publish=lambda *_, **__: None
    )
    uow.bus = bus
    return bus


def random_sku():
    return uuid.uuid4().hex[:6]


def test_out_of_stock_email(bus):
    sku = random_sku()
    bus.handle(commands.CreateBatch('batch1', sku, 9, None))
    bus.handle(commands.Allocate('order1', sku, 10))
    messages = requests.get(
        f'http://{cfg["host"]}:{cfg["http_port"]}/api/v2/messages'
    ).json()
    message = next(
        m for m in messages['items']
        if sku in str(m)
    )
    assert message['Raw']['From'] == 'allocations@example.com'
    assert message['Raw']['To'] == ['stock@made.com']
    assert f'Out of stock for {sku}' in message['Raw']['Data']
----
====

against all the odds this actually worked, pretty much first go!


And, erm, that's it really.

1. Define your API using an ABC
2. Implement the real thing
3. Build a fake and use it for unit / service-layer / handler tests
4. Find a less-fake version you can put into your docker environment
5. Test the less-fake "real" thing
6. Profit!


.Exercise for the Reader 
******************************************************************************
NOTE: TODO, under construction

Why not have a go at changing from email to, idk, twilio or slack
notifications or something?

Oh yeah, step 4 is a bit challenging...

Or, do the same thing for redis.  You'll need to split pub from sub.
******************************************************************************


=== DI wrap-up

* messagebus is a nice place to do DI. it becomes a composition root
* but you don't have to.  it is a violation of the SRP after all. if you use
  a <<appendix_bootstrap,bootstrap script>>, that can be a good place to
  initialise some alternative DI tools
* off-the-shelf, you could try https://pypi.org/project/Inject/[Inject]
  (it's fine, we use it at MADE, it makes pylint complain) or 
  https://pypi.org/project/punq/[Punq] (as written by Bob himself).

TODO: expand on wrap-up

