[preface]
[[chapter_02B_abstractions]]
== A Brief Interlude: On Coupling and Abstractions

.In this chapter
* We'll discuss how to reduce coupling by introducing simple abstractions over
  complex subsystems.
* We'll show how to make complex processes testable by disentangling state from
  logic.

// this chapter is marked as a preface as a hack to disable
// chapter numbering for it in atlas/pdf.  when we go to prod
// we should find a better solution.

// (ej) Looks like some of the intro and concluding text still need updating
// for the move, but I think this is a good candidate for Chapter 1.
//
// If you closed out this chapter with some of the content from the Domain Modeling chapter,
// it could serve as a gentle intro to Ch2 Domain Modeling.
//
//  Hypothetically, a flow that makes sense to me is:
//  Ch1: Abstractions (or maybe Complexity?)
//  - What are the causes of complexity in software? (changing requirements,
//      turnover in teams, conway's law, evolving technologies etc., etc.)
//  - BBOM
//  - How do we manage complexity? -> By organizing code into abstractions and modules.
//  - Content on the filesystem examples, fcis, di, etc. as examples of different styles of abstraction
//  - How do you know what makes a "good" abstraction, or a "good" module decomposition?
//  - Discussion of coupling and cohesion, sep-of-concerns, information hiding principle. (Information
//    hiding pinciple suggests that modules/abstractions should be designed
//    to *hide/encapsulate the things that are most likely to change.*)
//  - 3-layer architecture as a form of information hiding and abstraction.
//  - DDD and domain modeling as a way of choosing abstractions.
//
//  Ch 2. Domain modeling
//  - Jump straight into DDD patterns and product/warehouse modeling.
//

Allow us a brief digression on the subject of abstractions, dear reader.
We've talked about _abstractions_ quite a lot.  The repository is an
abstraction over permanent storage for example.  But what makes a good
abstraction?  What do we want from them?

A key theme in this book, hidden among the fancy patterns, is that we can use
simple abstractions to hide messy details. When we're writing code for fun, or
in a kata, we get to play with ideas freely, hammering things out and
refactoring aggressively. In a large-scale system, though, we become constrained
by the decisions made elsewhere in the system.
// TODO (DS): Should you say what a kata is?

When we're unable to change component A for fear of breaking component B, we say
that the components have become coupled. Locally, coupling is a good thing: it's
a sign that our code is working together, each component supporting the others,
fitting in place like the gears of a watch.

Globally, coupling is a nuisance: it increases the risk and the cost of changing
our code, sometimes to the point where we feel unable to make some changes at
all. This is the problem with the ball of mud pattern: as the application grows,
the coupling increases superlinearly until we are no longer able to effectively
change our systems.

// (ej) I'm reading the preceding two paragraphs as essentially describing coupling vs. cohesion,
//      where "local" coupling implies high cohesion, "global coupling" implies low cohesion.
//      using those terms specifically will let readers google for more info.

We can reduce the degree of coupling within a system
(<<coupling_illustration1>>) by abstracting away the details
(<<coupling_illustration2>>):


[[coupling_illustration1]]
.Lots of coupling
image::images/coupling_illustration1.png[]

[[coupling_illustration2]]
.Less coupling
image::images/coupling_illustration2.png[]


In both diagrams, we have a pair of subsystems, with the one dependent on
the other. In the first diagram, there is a high degree of coupling between the
two because of reasons. If we need to change system B, there's a good
chance that the change will ripple through to system A.

In the second, though, we have reduced the degree of coupling by inserting a
new, simpler, abstraction. This abstraction serves to protect us from change by
hiding away the complex details of whatever system B does.

// (ej)
//       I'm a bit of a stickler on semantics of diagrams, but I'm not
//       sure how to interpret the pictures, as they're too abstract.
//
//       In ASCII form, these are:
//       - A <-> B
//       - A <-> Abstraction <-> B
//
//        The double-ended arrow to me implies circular dependency, which means A and B are still
//        coupled in the above diagrams.
//
//       For A and B to be decoupled, the pictures I see in my mind are one of these dependency relationships:
//       - A -> Abstraction -> B
//       - A <- Abstraction <- B
//       - A -> Abstraction <- B
//       - A <- Abstraction -> B

=== Abstracting State Aids Testability

Let's see an example. Imagine we want to write some code for synchronising two
file directories which we'll call the source and the destination.

* If a file exists in the source, but not the destination, copy the file over.
* If a file exists in the source, but has a different name than in the destination,
  rename the destination file to match.
* If a file exists in the destination but not the source, remove it.

Our first and third requirements are simple enough, we can just compare two
lists of paths. Our second is trickier, though. In order to detect renames,
we'll have to inspect the content of files. For this we can use a hashing
function like md5 or SHA.


A gallon of coffee later, we've got a working prototype, and it looks
something like this:

////
TODO (DS)
This is a pattern we see a lot in the book: giving an example of the less good, then showing a better way.

The problem is you're asking your readers to put effort into understanding
something you don't even recommend doing. Also, because this less good way of
writing things is the more obvious way, the readers will be very familiar with
it already.

Personally I'd prefer to see an example of good practice from the get go. You
could talk about what it helps you avoid, but it doesn't need detailed code
samples. 

Also, a good way to approach writing abstracted code is to start with the
abstraction anyway, only later writing the implementation. Explaining things in
that order would demonstrate nicely that abstractions allow you to defer
decisions about implementation until a late stage...
////

[[sync_first_cut]]
.Basic sync algorithm (sync.py)
====
[source,python]
[role="non-head"]
----
import hashlib
import os
import shutil
from pathlib import Path

def sync(source, dest):
    source_hashes = {}  #<1>
    for folder, _, files in os.walk(source):
        for fn in files:
            source_hashes[hash_file(Path(folder) / fn)] = fn  #<1>

    seen = set()  #<5>
    for folder, _, files in os.walk(dest):  #<2>
        for fn in files:
            dest_path = Path(folder) / fn
            dest_hash = hash_file(dest_path)
            seen.add(dest_hash)

            if dest_hash not in source_hashes:
                dest_path.remove()  #<3>
            elif dest_hash in source_hashes and fn != source_hashes[dest_hash]:
                shutil.move(dest_path, Path(folder) / source_hashes[dest_hash])  #<4>

    for src_hash, fn in source_hashes.items():
        if src_hash not in seen:
            shutil.copy(Path(source) / fn, Path(dest) / fn)  #<5>
----
====

<1> We first walk the source folder and build up a dictionary of hashes to
    relative file paths.

<2> We then walk the target folder and hash its files, in order to determine

<3> If there are any hashes we don't recognise, we delete the file,

<4> Or if there is a hash we recognise but the filename doesn't match, we
    do a rename.

<5> Finally we run through any files in the source that we haven't seen
    in the destination and we copy them.

// TODO (DS): Might be easier to understand if you broke sync into a few
// private functions.

// TODO (DS): I read fn as function the first time. Would filename be better?

// TODO (DS): What's hash_file and how does it relate to hashlib?
// Edit: now i see below, but it confused me at this point.

// TODO (DS): I think you'd need less of these footnotes with a few comments or
// some clearer factoring

`hash_file` just returns the SHA-1 hash for a given path:


[[hash_file]]
.Hashing a file (sync.py)
====
[source,python]
----
BLOCKSIZE = 65536

def hash_file(path):
    hasher = hashlib.sha1()  #<2>
    with path.open("rb") as file:
        buf = file.read(BLOCKSIZE)
        while buf:
            hasher.update(buf)
            buf = file.read(BLOCKSIZE)
    return hasher.hexdigest()
----
====


Fantastic! We have some code and it _looks_ okay, but before we run it on our
hard drive, maybe we should test it?  How do we go about testing this sort of thing?


[[ugly_sync_tests]]
.Some end-to-end tests (test_sync.py)
====
[source,python]
[role="non-head"]
----
def test_when_a_file_exists_in_the_source_but_not_the_destination():
    try:
        source = tempfile.mkdtemp()
        dest = tempfile.mkdtemp()

        content = "I am a very useful file"
        (Path(source) / 'my-file').write_text(content)

        sync(source, dest)

        expected_path = Path(dest) /  'my-file'
        assert expected_path.exists()
        assert expected_path.read_text() == content

    finally:
        shutil.rmtree(source)
        shutil.rmtree(dest)


def test_when_a_file_has_been_renamed_in_the_source():
    try:
        source = tempfile.mkdtemp()
        dest = tempfile.mkdtemp()

        content = "I am a file that was renamed"
        source_path = Path(source) / 'source-filename'
        old_dest_path = Path(dest) / 'dest-filename'
        expected_dest_path = Path(dest) / 'source-filename'
        source_path.write_text(content)
        old_dest_path.write_text(content)

        sync(source, dest)

        assert old_dest_path.exists() is False
        assert expected_dest_path.read_text() == content


    finally:
        shutil.rmtree(source)
        shutil.rmtree(dest)
----
====

Wowsers, that's a lot of setup for two very simple cases! The problem is that
our domain logic, "figure out the difference between two directories," is tightly
coupled to the IO code. We can't run our difference algorithm without calling
the pathlib, shutil, and hashlib modules.

// (ej)
//     As a motivating "what-if", at this point you could ask the following thought experiments:
//       1) What if you wanted to re-use the same code so this also works synchronizing remote servers?
//       2) What if you wanted to add a "dry-run" feature?
//       What extra complexity would these scenarios create?

Our high-level code is coupled to low-level details, and it's making life hard.
As the scenarios we consider get more complex, our tests will get more unwieldy.
We can definitely refactor these tests (some of the cleanup could go into pytest
fixtures for example) but as long as we're doing filesystem operations, they're
going to stay slow and hard to read and write.

=== Choosing the right abstraction(s)

What could we do to rewrite our code to make it more testable?

Firstly we need to think about what our code needs from the filesystem.
Reading through the code, there are really three distinct things happening.

1. We interrogate the filesystem using `os.walk` and determine hashes for a
   series of paths. This is actually very similar in both the source and the
   destination cases.

2. We decide a file is new, renamed, or redundant.

3. We copy, move, or delete, files to match the source.

What could we do to abstract out the filesystem in each case?

NOTE: In this chapter we're refactoring some gnarly code into a more testable
    structure by identifying the separate tasks that need to be done and giving
    each task to a clearly defined actor, along similar lines to the `duckduckgo`
    example from the prologue.

For (1) and (2), we've already intuitively started using an abstraction, a
dictionary of hashes to paths, and you may already have been thinking, "why not
use build up a dictionary for the destination folder as well as the source,
then we just compare two dicts?"  That seems like a very nice way to abstract
the current state of the filesystem.

    source_files = {'hash1': 'path1', 'hash2': 'path2'}
    dest_files = {'hash1': 'path1', 'hash2': 'pathX'}

// TODO (DS): That approach could have been used in the non abstracted version
// maybe? 

What about moving from step (2) to step (3)?  How can we abstract out the
actual move/copy/delete filesystem interaction?  This one is perhaps a little
less intuitive, but how about some sort of collection of strings?

    ("COPY", "sourcepath", "destpath"),
    ("MOVE", "old", "new"),

////
TODO (DS): Difficult to follow this bit...

It feels like you're making an implicit concept explicit: modelling the
*intent* of the system.

But is that really the same as using abstractions? It feels like you're doing
two things at once here: moving away from an imperative style to one that is
more declarative, *and* depending on abstractions instead of implementations.
Those are kind of orthogonal.... Looking at one of these at a time might be
clearer.
////

Now we could write tests that just use 2 filesystem dicts as inputs, and
expect lists of tuples of strings representing actions as outputs.

Instead of saying "given this actual filesystem, when I run my function,
check what actions have happened?" we say, "given this _abstraction_ of a filesystem,
what _abstraction_ of filesystem actions will happen?"


[[better_tests]]
.Simplified inputs and outputs in our tests (test_sync.py)
====
[source,python]
[role="skip"]
----
    def test_when_a_file_exists_in_the_source_but_not_the_destination():
        src_hashes = {'hash1': 'fn1'}
        dst_hashes = {}
        expected_actions = [('COPY', '/src/fn1', '/dst/fn1')]
        ...

    def test_when_a_file_has_been_renamed_in_the_source():
        src_hashes = {'hash1': 'fn1'}
        dst_hashes = {'hash1': 'fn2'}
        expected_actions == [('MOVE', '/dst/fn2', '/dst/fn1')]
        ...
----
====


=== Implementing our chosen abstractions

That's all very well, but how do we _actually_ write those new
tests, and how do we change our implementation to make it all work?

There are essentially two ways.  The first is to try and split out the core of
our "business logic" into a functional core and test that directly, pushing the
I/O out to a thin, imperative shell which we can either test with (few) end to
end tests, or make simple enough that we're happy to not test at all.

// TODO (DS): Really interesting. I wonder if the two approaches would be
// served by two contrasting diagrams.
// Also, maybe these two contrasting approaches to decoupling biz logic from
// implementation should be in the intro to the chapter...?

The second way would be to expose the IO dependencies in our top-level
function, and use dependency injection to swap out stub versions of our
abstraction for the tests, leaving the real versions for the real code.

Let's see them in turn.


==== Option 1 - Functional Core, Imperative Shell. Ish.

Let's call this the "Harry Way."  FCIS is probably a bit of an aspirational
name, in fact the point is not to have a pure-functional solution in the
sense of not-using-classes, but more in the sense of having no (or minimal)
side-effects.  The aim is to split out a core of business logic with no
dependencies, and test that separately from the rest of our system.

// (ej)
// Referring to the "Coupling" diagram comment previously, the snippet below
// would look like:
//
// determine_actions <- sync -> read_paths_and_hashes
//

[[three_parts]]
.Split our code into three  (sync.py)
====
[source,python]
----
def sync(source, dest):  #<3>
    source_hashes = read_paths_and_hashes(source)
    dest_hashes = read_paths_and_hashes(dest)
    actions = determine_actions(source_hashes, dest_hashes, source, dest)
    for action, *paths in actions:
        if action == 'copy':
            shutil.copyfile(*paths)
        if action == 'move':
            shutil.move(*paths)
        if action == 'delete':
            os.remove(paths[0])

...

def read_paths_and_hashes(root):  #<1>
    hashes = {}
    for folder, _, files in os.walk(root):
        for fn in files:
            hashes[hash_file(Path(folder) / fn)] = fn
    return hashes


def determine_actions(src_hashes, dst_hashes, src_folder, dst_folder):  #<2>
    for sha, filename in src_hashes.items():
        if sha not in dst_hashes:
            sourcepath = Path(src_folder) / filename
            destpath = Path(dst_folder) / filename
            yield 'copy', sourcepath, destpath

        elif dst_hashes[sha] != filename:
            olddestpath = Path(dst_folder) / dst_hashes[sha]
            newdestpath = Path(dst_folder) / filename
            yield 'move', olddestpath, newdestpath

    for sha, filename in dst_hashes.items():
        if sha not in src_hashes:
            yield 'delete', dst_folder / filename
----
====

<1> The code to build up the dictionary of paths and hashes is now trivially
    easy to write.

<2> The core of our "business logic," which says, "given these two sets of
    hashes and filenames, what should we copy/move/delete?"  takes simple
    data structures and returns simple data structures.

<3> And our top-level module now contains almost no logic whatseover


////
TODO (ej)
ej.1 Calling out the sections that are "shell" and "core" will give the terms some visual impact.
ej.2 I played around with refactoring this a bit, and stunbled across something interesting.

Source + dest are only necessary if you actually want to mutate the filesystem.
You could push the responsibility for forming the absolute paths out to the
"shell".

That then raises the question of whether this function is just detecting
changes in filesystem state, or determining the actions that should be
performed.  The flags could just as easily be {"file created", "file moved",
"file deleted"} instead of {"copy", "move", "delete"}.

IRL this would be unnecessary hair splitting, but for the purposes of this
chapter, maybe it will be useful for discussing modeling + abstractions, and
foreshadowing some of the event stuff in later chapters.  I am unsure if it
will fit into the flow of this text.


def ej_sync(source, dest):
    # Imperative shell, input  # ej.1
    source_hashes = read_paths_and_hashes(source)
    dest_hashes = read_paths_and_hashes(dest)

    # Functional core
    actions = ej_determine_actions(source_hashes, dest_hashes)

    # Imperative shell, output
    for action, *paths in actions:
        if action == 'copy':
            shutil.copyfile(Path(source)/path[0], Path(dest)/path[1])
        if action == 'move':
            shutil.move(Path(dest)/path[0], Path(dest)/path[1])
        if action == 'delete':
    ...


def ej_determine_actions(src_hashes, dst_hashes):  # ej.2
    for sha, filename in src_hashes.items():
        if sha not in dst_hashes:
            sourcepath = filename
            destpath = filename
            yield 'copy', sourcepath, destpath

        elif dst_hashes[sha] != filename:
            olddestpath = dst_hashes[sha]
            newdestpath =  filename
            yield 'move', olddestpath, newdestpath

    for sha, filename in dst_hashes.items():
        if sha not in src_hashes:
            yield 'delete', filename
////


Our tests now act directly on the `determine_actions()` function:


[[harry_tests]]
.Nicer looking tests (test_sync.py)
====
[source,python]
----
    @staticmethod
    def test_when_a_file_exists_in_the_source_but_not_the_destination():
        src_hashes = {'hash1': 'fn1'}
        dst_hashes = {}
        actions = list(determine_actions(src_hashes, dst_hashes, Path('/src'), Path('/dst')))
        assert actions == [('copy', Path('/src/fn1'), Path('/dst/fn1'))]

    @staticmethod
    def test_when_a_file_has_been_renamed_in_the_source():
        src_hashes = {'hash1': 'fn1'}
        dst_hashes = {'hash1': 'fn2'}
        actions = list(determine_actions(src_hashes, dst_hashes, Path('/src'), Path('/dst')))
        assert actions == [('move', Path('/dst/fn2'), Path('/dst/fn1'))]
----
====


Because we've disentangled the logic of our program - the code for identifying
changes - from the low-level details of IO, we can easily test the core of our code.

==== Option 2: Dependency Injection

Let's call this the "Bob way," and it's about making dependencies explicit and
modifiable:

//
// (ej) Referring back to the "coupling" diagram comment, the snippet below would look like:
//
// apply_func <- synchronize_dirs -> reader
//

[[di_version]]
.Explicit dependencies (sync.py)
====
[source,python]
[role="skip"]
----
def synchronise_dirs(reader, apply_func, src_folder, dst_folder):  #<1>
    src_hashes = reader(src_folder)  #<2>
    dst_hashes = reader(dst_folder)  #<2>

    for sha, filename in src_hashes.items():
        if sha not in dst_hashes:
            sourcepath = src_folder / filename
            destpath = dst_folder / filename
            apply_func('COPY', sourcepath, destpath)  #<3>

        elif dst_hashes[sha] != filename:
            olddestpath = dst_folder / dst_hashes[sha]
            newdestpath = dst_folder / filename
            apply_func('MOVE', olddestpath, newdestpath)  #<3>

    for sha, filename in dst_hashes.items():
        if sha not in src_hashes:
            apply_func('DELETE', dst_folder / filename)  #<3>

----
====

// TODO (DS): Why not just call reader `read_paths_and_hashes`, like in the
// FCIS example?

//NICE-TO-HAVE: test this listing

<1> Our top-level function now exposes two new dependencies, a `reader` and an
    `apply_func`

<2> We invoke the `reader` to produce our dict-abstraction of the filesystems

<3> And we invoke the `apply_func` using our action-abstraction for the actions
    we want to apply.

TIP: Notice that, although we're using dependency injection, there was no need
    to define an abstract base class or any kind of explicit interface.  In the
    book we often show ABCs because we hope they help to understand what the
    abstraction is, but they're not necessary.  Python's dynamic nature means
    we can always rely on duck typing.

////
TODO (DS)
Really, the only difference between the DI approach shown here and the FCIS is
that in DI, the orchestration logic can be tested against abstractions too (at
the cost of some extra indirection).

It feels to me that this DI approach is building on the FCIS, rather than
demonstrating the 'pure' version of DI. In real life, it's nice to combine
them, but perhaps it would be a clearer comparison if you showed a purer
version of each.

I also think you might need to address how the patterns you propose in this
book relate to the alternative approaches. To me, it seems the core patterns
are much more DI than FCIS. But you could also point out that the approaches
can be combined - it's not either/or.
////

////
TODO (e.j)
Played around with this a bit, and I think introducing an explicit filesystem
abstraction helps contrast the DI vs FCIS styles more strongly.

It also make the duck-typing comment more relevant, and could be used to
introduce the idea of roles and encapsulation.

def ej_synchronise_dirs(reader, filesystem, source_root, dest_root):
    source_hashes = reader(source_root)
    dest_hashes = reader(dest_root)

    for sha, filename in src_hashes.items():
        if sha not in dst_hashes:
            sourcepath = source_root / filename
            destpath = dest_root / filename
            filesystem.copy(destpath, sourcepath)

        elif dst_hashes[sha] != filename:
            olddestpath = dest_root / dst_hashes[sha]
            newdestpath = dest_root / filename
            filesystem.move(oldestpath, newdestpath)

    for sha, filename in dst_hashes.items():
        if sha not in src_hashes:
            filesystem.del(dest_root/filename)
////

[[bob_tests]]
.Tests using DI
====
[source,python]
[role="skip"]
----
def test_when_a_file_exists_in_the_source_but_not_the_destination():
    source = {"sha1": "my-file" }
    dest = {}
    actions = []

    reader = {"/source": source, "/dest": dest}
    synchronise_dirs(reader.pop, actions.append, "/source", "/dest")

    assert actions == [("COPY", "/source/my-file", "/dest/my-file")]


def test_when_a_file_has_been_renamed_in_the_source():
    source = {"sha1": "renamed-file" }
    dest = {"sha1": "original-file" }
    actions = []

    reader = {"/source": source, "/dest": dest}
    synchronise_dirs(reader.pop, actions.append, "/source", "/dest")

    assert actions == [("MOVE", "/dest/original-file", "/dest/renamed-file")]
----
====

////
TODO (ej)
Modified this a bit to also introduce the filesystem abstraction.

class FakeFilesystem(object):
    def __init__(self, actions):
        self._actions = actions or []

    def copy(self, src, dest):
        self._actions.append(('COPY', src, dest))

    def move(self, src, dest):
        self._actions.append(('MOVE', src, dest))

    def delete(self, dest):
        self._actions.append(('DELETE', src, dest))


then


     actions = [] # Using the shunt pattern here.

////



The advantage of this approach is that your tests act on the exact same function
that's used by your production code.  The disadvantage is that DI usually demands
a bit more work on the part of the reader to understand what's going on.

In either case, we can now work on fixing all the bugs in our implementation;
enumerating tests for all the edge cases is now much easier.


.So which do we use in this book? FCIS or DI?
******************************************************************************
Both. Our domain model is entirely free of dependencies and side-effects,
so that's our functional core.  The service layer that we build around it
(in <<chapter_03_service_layer>>) is its imperative shell, but we actually
use dependency injection to provide that imperative shell with things like
access to the database, so we can still unit test it.

See <<chapter_10_dependency_injection>> for more exploration of making our
dependency injection more explicit and centralised.
******************************************************************************

=== Wrap-up: "Depend on Abstractions."

We'll see this idea come up again and again in the book: we can make our
systems easier to test and maintain by simplifying the interface between our
business logic and messy IO. Finding the right abstraction is tricky, but here's
a few heuristics and questions to ask yourself:


* Can I choose a familiar Python datastructure to represent the state of the
  messy system, and try to imagine a single function that can return that
  state?
// TODO (DS): These are great heuristics... Maybe they deserve more attention?

* Where can I draw a line between my systems, where can I carve out a seam, to
  stick that abstraction in?

// TODO (DS): Drawing lines and the dependencies between them is really
// relevant to what you've done in this chapter, but i don't think you've
// explicitly addressed them except in this bullet point.

// TODO (DS): I think the seam metaphor might need more explanation.
// (I assume this is taken from Michael Feathers? I've always been confused
// about whether it's a sewing seam, or a mining seam!)

// TODO (DS): And maybe, which implicit concepts can i make explicit?

* What are the dependencies and what is the core "business" logic?



Practice makes less-imperfect!

// TODO (DS): I think this is potentially a great chapter, perhaps belonging
// really on in the book. But it is also a bit of a brain dump of lots of deep,
// amazing concepts. I don't think you've quite found the best structure here
// yet. Perhaps it could be structured around these heuristics?

And now back to our regular programming...
