[[chapter_04_service_layer]]

== Our First Use Case: Flask API and Service Layer

Back to our allocations project!

In this chapter, we discuss the difference between orchestration logic,
business logic, and interfacing code, and we introduce the _Service Layer_
pattern to take care of orchestrating our workflows and defining the use
cases of our system.

We'll also discuss testing: by combining the Service Layer with our Repository
abstraction over the database, we're able to write fast tests, not just of
our domain model, but the entire workflow for a use case.

By the end of this chapter, we'll have added a Flask API that will talk to
the Service Layer, which will serve as the entrypoint to our Domain Model.
By making the service layer depend on the `AbstractRepository`, we'll be
able to unit test it using `FakeRepository`, and then run it in real life
using `SqlAlchemyRepository`.  <<service_layer_class_diagram>> is a class
diagram showing where we're heading.

[[service_layer_class_diagram]]
.Our target architecture for the end of this chapter
image::images/service_layer_class_diagram.png[]
[role="image-source"]
----
[plantuml, service_layer_class_diagram, config=plantuml.cfg]
@startuml
hide empty members
left to right direction

package "Flask API" as api {

  class "allocate_endpoint()" as allocate_endpoint {
  }
  hide allocate_endpoint circle
}


package allocation {

  package services {
    class "allocate(line, repository, session)" as allocate {
    }
    hide allocate circle
  }

  abstract class AbstractRepository {
    add ()
    get ()
    list ()
  }

  class Batch {
    allocate ()
  }


  class FakeRepository {
    batches: List[Batch]
  }

  class BatchRepository {
    session: Session
  }

}

package sqlalchemy {
  class Session {
    query()
    add()
  }
}


allocate -> AbstractRepository: uses
AbstractRepository -> Batch : stores

AbstractRepository <|-- FakeRepository : implements
AbstractRepository <|-- BatchRepository : implements
allocate_endpoint --> allocate : invokes

BatchRepository --> Session : abstracts
@enduml
----


[TIP]
====
You can find our code for this chapter at
https://github.com/cosmicpython/code/tree/chapter_04_service_layer[github.com/cosmicpython/code/tree/chapter_04_service_layer].

----
git clone https://github.com/cosmicpython/code.git && cd code
git checkout chapter_04_service_layer
# or, if you want to code along, checkout the previous chapter:
git checkout chapter_02_repository
----
====


=== Connecting Our Application to the Real World

Like any good agile team, we're hustling to try and get an MVP out and
in front of the users to start gathering feedback.  We have the core
of our domain model and the domain service we need to allocate orders,
and we have the Repository interface for permanent storage.

Let's try and plug all the moving parts together as quickly as we
can, and then refactor towards a cleaner architecture.  Here's our
plan:

1. Use Flask to put an API endpoint in front of our `allocate` domain service.
   Wire up the database session and our repository.  Test it with
   an end-to-end test and some quick and dirty SQL to prepare test
   data.

2. Refactor out a Service Layer to serve as an abstraction to
   capture the use case, and sit between Flask and our Domain Model.
   Build some service-layer tests and show how they can use the
   `FakeRepository`.

3. Experiment with different types of parameters for our service layer
   functions; show that using primitive data types allows the service-layer's
   clients (our tests and our flask API) to be decoupled from the model layer.

4. Add an extra service called `add_stock` so that our service-layer
   tests and end-to-end tests no longer need to go directly to the
   storage layer to set up test data.


=== A First End-To-End (E2E) Test

No-one is interested in getting into a long terminology debate about what
counts as an E2E test versus a functional test versus an acceptance test versus
an integration test versus unit tests.  Different projects need different
combinations of tests, and we've seen perfectly successful projects just split
things into "fast tests" and "slow tests."

For now we want to write one or maybe two tests that are going to exercise
a "real" API endpoint (using HTTP) and talk to a real database. Let's call
them end-to-end tests because it's one of the most self-explanatory names.

<<first_api_test>> shows a first cut:

[[first_api_test]]
.A first API test (test_api.py)
====
[source,python]
[role="non-head"]
----
@pytest.mark.usefixtures('restart_api')
def test_api_returns_allocation(add_stock):
    sku, othersku = random_sku(), random_sku('other')  #<1>
    batch1, batch2, batch3 = random_batchref(1), random_batchref(2), random_batchref(3)
    add_stock([  #<2>
        (batch1, sku, 100, '2011-01-02'),
        (batch2, sku, 100, '2011-01-01'),
        (batch3, othersku, 100, None),
    ])
    data = {'orderid': random_orderid(), 'sku': sku, 'qty': 3}
    url = config.get_api_url()  #<3>
    r = requests.post(f'{url}/allocate', json=data)
    assert r.status_code == 201
    assert r.json()['batchref'] == batch2
----
====

<1> `random_sku()`, `random_batchref()` etc are little helper functions that
    add generate some randomized characters using the `uuid` module.  Because
    we're running against an actual database now, this is one way to prevent
    different tests and runs from interfering with each other.

<2> `add_stock` is a helper fixture that just hides away the details of
    manually inserting rows into the database using SQL.  We'll find a nicer
    way of doing this later in the chapter.

<3> _config.py_ is a module for getting configuration information.  Again,
    this is an unimportant detail, and everyone has different ways of
    solving these problems, but if you're curious, you can find out more
    in <<appendix_project_structure>>.

Everyone solves these problems in different ways, but you're going to need some
way of spinning up Flask, possibly in a container, and also talking to a
postgres database.  If you want to see how we did it, check out
<<appendix_project_structure>>.


=== The Straightforward Implementation

Implementing things in the most obvious way, you might get something like this:


[[first_cut_flask_app]]
.First cut Flask app (flask_app.py)
====
[source,python]
[role="non-head"]
----
from flask import Flask, jsonify, request
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

import config
import model
import orm
import repository


orm.start_mappers()
get_session = sessionmaker(bind=create_engine(config.get_postgres_uri()))
app = Flask(__name__)

@app.route("/allocate", methods=['POST'])
def allocate_endpoint():
    session = get_session()
    batches = repository.SqlAlchemyRepository(session).list()
    line = model.OrderLine(
        request.json['orderid'],
        request.json['sku'],
        request.json['qty'],
    )

    batchref = model.allocate(line, batches)

    return jsonify({'batchref': batchref}), 201
----
====

//TODO (hynek) pretty sure you can drop the jsonify call

So far so good.  No need for too much more of your "architecture astronaut"
nonsense, Bob and Harry, you may be thinking.

But hang on a minute--there's no commit.  We're not actually saving our
allocation to the database. Now we need a second test, either one that will
inspect the database state after (not very black-boxey), or maybe one that
checks that we can't allocate a second line if a first should have already
depleted the batch:

[[second_api_test]]
.Test allocations are persisted (test_api.py)
====
[source,python]
[role="non-head"]
----
@pytest.mark.usefixtures('restart_api')
def test_allocations_are_persisted(add_stock):
    sku = random_sku()
    batch1, batch2 = random_batchref(1), random_batchref(2)
    order1, order2 = random_orderid(1), random_orderid(2)
    add_stock([
        (batch1, sku, 10, '2011-01-01'),
        (batch2, sku, 10, '2011-01-02'),
    ])
    line1 = {'orderid': order1, 'sku': sku, 'qty': 10}
    line2 = {'orderid': order2, 'sku': sku, 'qty': 10}
    url = config.get_api_url()

    # first order uses up all stock in batch 1
    r = requests.post(f'{url}/allocate', json=line1)
    assert r.status_code == 201
    assert r.json()['batchref'] == batch1

    # second order should go to batch 2
    r = requests.post(f'{url}/allocate', json=line2)
    assert r.status_code == 201
    assert r.json()['batchref'] == batch2
----
====

Not quite so lovely, but that will force us to get a commit in.



=== Error Conditions That Require Database Checks

If we keep going like this though, things are going to get uglier and uglier.

Supposing we want to add a bit of error-handling.  What if the domain raises an
error, for a sku that's out of stock?  Or what about a sku that doesn't even
exist? That's not something the domain even knows about, nor should it.  It's
more of a sanity-check that we should implement at the database layer, before
we even invoke the domain service.

Now we're looking at two more end-to-end tests:


[[test_error_cases]]
.Yet more tests at the e2e layer...  (test_api.py)
====
[source,python]
[role="non-head"]
----
@pytest.mark.usefixtures('restart_api')
def test_400_message_for_out_of_stock(add_stock):  #<1>
    sku, smalL_batch, large_order = random_sku(), random_batchref(), random_orderid()
    add_stock([
        (smalL_batch, sku, 10, '2011-01-01'),
    ])
    data = {'orderid': large_order, 'sku': sku, 'qty': 20}
    url = config.get_api_url()
    r = requests.post(f'{url}/allocate', json=data)
    assert r.status_code == 400
    assert r.json()['message'] == f'Out of stock for sku {sku}'


@pytest.mark.usefixtures('restart_api')
def test_400_message_for_invalid_sku():  #<2>
    unknown_sku, orderid = random_sku(), random_orderid()
    data = {'orderid': orderid, 'sku': unknown_sku, 'qty': 20}
    url = config.get_api_url()
    r = requests.post(f'{url}/allocate', json=data)
    assert r.status_code == 400
    assert r.json()['message'] == f'Invalid sku {unknown_sku}'
----
====

<1> In the first test we're trying to allocate more units than we have in stock

<2> In the second, the sku just doesn't exist (because we never called `add_stock`),
    so it's invalid as far as our app is concerned.


And, sure we could implement it in the Flask app too:

[[flask_error_handling]]
.Flask app starting to get crufty (flask_app.py)
====
[source,python]
[role="non-head"]
----
def is_valid_sku(sku, batches):
    return sku in {b.sku for b in batches}

@app.route("/allocate", methods=['POST'])
def allocate_endpoint():
    session = get_session()
    batches = repository.SqlAlchemyRepository(session).list()
    line = model.OrderLine(
        request.json['orderid'],
        request.json['sku'],
        request.json['qty'],
    )

    if not is_valid_sku(line.sku, batches):
        return jsonify({'message': f'Invalid sku {line.sku}'}), 400

    try:
        batchref = model.allocate(line, batches)
    except model.OutOfStock as e:
        return jsonify({'message': str(e)}), 400

    session.commit()
    return jsonify({'batchref': batchref}), 201
----
====

But our Flask app is starting to look a bit unwieldy.  And our number of
E2E tests is starting to get out of control, and soon we'll end up with an
inverted test pyramid (or "ice cream cone model" as Bob likes to call it).


=== Introducing a Service Layer, and Using FakeRepository to Unit Test It

If we look at what our Flask app is doing, there's quite a lot of what we
might call __orchestration__—fetching stuff out of our repository, validating
our input against database state, handling errors, and committing in the
happy path.  Most of these things aren't anything to do with having a
web API endpoint (you'd need them if you were building a CLI for example, see
<<appendix_csvs>>), and they're not really things that need to be tested by
end-to-end tests.

It often makes sense to split out a Service Layer, sometimes called
_orchestration layer_ or _use case layer_.

Do you remember the `FakeRepository` that we prepared in the last chapter?

[[fake_repo]]
.Our fake repository, an in-memory collection of Batches (test_services.py)
====
[source,python]
----
class FakeRepository(repository.AbstractRepository):

    def __init__(self, batches):
        self._batches = set(batches)

    def add(self, batch):
        self._batches.add(batch)

    def get(self, reference):
        return next(b for b in self._batches if b.reference == reference)

    def list(self):
        return list(self._batches)
----
====

Here's where it will come in useful; it lets us test our service layer with
nice, fast unit tests:


[[first_services_tests]]
.Unit testing with fakes at the services layer (test_services.py)
====
[source,python]
[role="non-head"]
----
def test_returns_allocation():
    line = model.OrderLine("o1", "COMPLICATED-LAMP", 10)
    batch = model.Batch("b1", "COMPLICATED-LAMP", 100, eta=None)
    repo = FakeRepository([batch])  #<1>

    result = services.allocate(line, repo, FakeSession())  #<2><3>
    assert result == "b1"


def test_error_for_invalid_sku():
    line = model.OrderLine("o1", "NONEXISTENTSKU", 10)
    batch = model.Batch("b1", "AREALSKU", 100, eta=None)
    repo = FakeRepository([batch])  #<1>

    with pytest.raises(services.InvalidSku, match="Invalid sku NONEXISTENTSKU"):
        services.allocate(line, repo, FakeSession())  #<2><3>
----
====


<1> `FakeRepository` holds the `Batch` objects that will be used by our test.

<2> Our services module (_services.py_) will define an `allocate()`
    service-layer function. It will sit between our `allocate_endpoint()`
    function in the API layer and the `allocate()` domain service function from
    our domain model.footnote:[Service-layer services and domain services do have
    confusingly similar names. We have a sidebar on it later in the chapter
    <<why_is_everything_a_service>>.].

<3> We also need a `FakeSession` to fake out the database session, see below:


[[fake_session]]
.A fake database session (test_services.py)
====
[source,python]
----
class FakeSession():
    committed = False

    def commit(self):
        self.committed = True
----
====

(The fake session is only a temporary solution.  We'll get rid of it and make
things even nicer in the next chapter, <<chapter_05_uow>>)

.Mocks vs Fakes; Classic Style vs London School TDD
*******************************************************************************

// TODO (hynek) this should be unified with the previous mock-hating section.
// also the semicolon looks weird.

Couldn't we have used a mock (from `unittest.mock`) instead of building our
own `FakeSession`, or instead of `FakeRepository`?  What's the difference
between a fake and a mock anyway?

We tend to find that building our own fakes is an excellent way of exercising
design pressure against our abstractions.  If our abstractions are nice and
simple, then they should be easy to fake.

In fact in the case of `FakeRepository`, because our fake has actual behavior,
using a magic mock from `unittest.mock` wouldn't really help.

In the case of `FakeSession`, the `session` object isn't one of our own
abstractions, so the argument doesn't apply;  in fact, a `unittest.mock` mock
would have been just fine, but out of habit we avoided using one;  in any case,
we'll be getting rid of it in the next chapter.

//TODO (hynek) you could mention Alex Gaynor's `pretend` which gives you
// stubs without mocks error-prone magic.

In general we try and avoid using mocks, and the associated `mock.patch`.
Whenever we find ourselves reaching for them, we often see it as an indication
that something is missing from our design.  You'll see a good example of that
in <<chapter_07_events_and_message_bus>> when we mock out an email-sending
module, but eventually we replace it with an explicit bit of dependency injection.
That's discussed in <<chapter_12_dependency_injection>>.

Regarding the definition of fakes vs mocks, the short but simplistic answer is:

* Mocks are used to verify _how_ something gets used;  they have methods
  like `assert_called_once_with()`.  They're associated with London-school
  TDD.

* Fakes are working implementations of the thing they're replacing, but
  they're only designed for use in tests; they wouldn't work "in real life",
  like our in-memory repository. But you can use them to make assertions about
  the end state of a system, rather than the behaviors along the way, so
  they're associated with classic-style TDD.

(We're slightly conflating mocks with spies and fakes with stubs here, and you
can read the long, correct answer in Martin Fowler's classic essay on the subject
called https://martinfowler.com/articles/mocksArentStubs.html[Mocks aren't Stubs])

(It also probably doesn't help that the `MagicMock` objects provided by
`unittest.mock` aren't, strictly speaking, mocks, they're spies if anything.
But they're also often used as stubs or dummies.  There, promise we're done with
the test double terminology nitpicks now.)

What about London-school vs classic-style TDD?  You can read more about those
two in Martin Fowler's article just cited, as well as https://softwareengineering.stackexchange.com/questions/123627/what-are-the-london-and-chicago-schools-of-tdd[on stackoverflow],
but in this book we're pretty firmly in the classicist camp.  We like to
build our tests around state, both in setup and assertions, and we like
to work at the highest level of abstraction possible rather than doing
checks on the behavior of intermediary collaborators.footnote:[
Which is not to say that we think the London School people are wrong. There
are some insanely smart people that work that way.  It's just not what we're
used to].

Read more on this shortly, in the <<kinds_of_tests,"high gear vs low gear">> section.

*******************************************************************************

The fake `.commit()` lets us migrate a third test from the E2E layer:


[[second_services_test]]
.A second test at the service layer (test_services.py)
====
[source,python]
[role="non-head"]
----
def test_commits():
    line = model.OrderLine('o1', 'OMINOUS-MIRROR', 10)
    batch = model.Batch('b1', 'OMINOUS-MIRROR', 100, eta=None)
    repo = FakeRepository([batch])
    session = FakeSession()

    services.allocate(line, repo, session)
    assert session.committed is True
----
====


==== A Typical Service Function

We'll get to a service function that looks something like <<service_function>>:

[[service_function]]
.Basic allocation service (services.py)
====
[source,python]
[role="non-head"]
----
class InvalidSku(Exception):
    pass


def is_valid_sku(sku, batches):  #<2>
    return sku in {b.sku for b in batches}

def allocate(line: OrderLine, repo: AbstractRepository, session) -> str:
    batches = repo.list()  #<1>
    if not is_valid_sku(line.sku, batches):  #<2>
        raise InvalidSku(f'Invalid sku {line.sku}')
    batchref = model.allocate(line, batches)  #<3>
    session.commit()  #<4>
    return batchref
----
====

Typical service-layer functions have similar steps:

<1> We fetch some objects from the repository

<2> We make some checks or assertions about the request against
    the current state of the world

<3> We call a domain service

<4> And if all is well, we save/update any state we've changed.

That last step is a little unsatisfactory at the moment, our services
layer is tightly coupled to our database layer, but we'll improve on
that in the next chapter.


[[depend_on_abstractions]]
."Depend on Abstractions"
*******************************************************************************
Notice one more thing about our service-layer function:

.The service depends on an abstraction (services.py)
====
[source,python]
[role="skip"]
----
def allocate(line: OrderLine, repo: AbstractRepository, session) -> str:  #<1>
----
====

It depends on a repository.  We've chosen to make the dependency explicit,
and we've used the type hint to say that we depend on
pass:[<code>AbstractRepository</code>]footnote:[
Is this Pythonic?  Depending on who you ask, both abstract base classes and
type hints are hideous abominations, and serve only to add useless, unreadable
cruft to your code; beloved only by people who wish that Python was Haskell,
which it will never be.  "beautiful is better than ugly," "simple is better
than complex," and "readability counts..."
Or, perhaps they make explicit something that would otherwise be implicit
("explicit is better than implicit").  For the purposes of this book, we've
decided this argument carries the day. What you decide to do in your own
codebase is up to you.]
This means it'll work both when the tests give it a `FakeRepository`, and
when the flask app gives it a `SqlAlchemyRepository`.

// TODO (hynek, re type-hating footnote: I feel like this meditation has already
// come up a few times.  Maybe have a short section instead to get it out of
// your system and be done?
// Also: type haters have never had to maintain large apps over a long time.

If you remember the <<dip,Dependency Inversion Principle section from the introduction>>,
this is what we mean when we says we should "depend on abstractions". Our
_high-level module_, the service layer, depends on the repository abstraction.
And the _details_ of the implementation for our specific choice of persistent
storage also depend on that same abstraction.

See the diagrams at the end of the chapter,
<<service_layer_diagram_abstract_dependencies>>.

See also <<appendix_csvs>> where we show a worked example of swapping out the
_details_ of which persistent storage system to use, while leaving the
abstractions intact.

*******************************************************************************


Still, the essentials of the services layer are there, and our Flask
app now looks a lot cleaner, <<flask_app_using_service_layer>>:


[[flask_app_using_service_layer]]
.Flask app delegating to service layer (flask_app.py)
====
[source,python]
[role="non-head"]
----
@app.route("/allocate", methods=['POST'])
def allocate_endpoint():
    session = get_session()  #<1>
    repo = repository.SqlAlchemyRepository(session)  #<1>
    line = model.OrderLine(
        request.json['orderid'],  #<2>
        request.json['sku'],  #<2>
        request.json['qty'],  #<2>
    )
    try:
        batchref = services.allocate(line, repo, session)  #<2>
    except (model.OutOfStock, services.InvalidSku) as e:
        return jsonify({'message': str(e)}), 400  <3>

    return jsonify({'batchref': batchref}), 201  <3>
----
====

We see that the responsibilities of the Flask app are much more minimal, and
more focused on just the web stuff:

<1> We instantiate a database session and some repository objects.
<2> We extract the user's commands from the web request and pass them
    to a domain service.
<3> And we return some JSON responses with the appropriate status codes

The responsibilities of the Flask app are just standard web stuff: per-request
session management, parsing information out of POST parameters, response status
codes and JSON.  All the orchestration logic is in the use case / service layer,
and the domain logic stays in the domain.


Finally we can confidently strip down our E2E tests to just two, one for
the happy path and one for the unhappy path:


[[fewer_e2e_tests]]
.E2E tests now only happy + unhappy paths (test_api.py)
====
[source,python]
[role="non-head"]
----
@pytest.mark.usefixtures('restart_api')
def test_happy_path_returns_201_and_allocated_batch(add_stock):
    sku, othersku = random_sku(), random_sku('other')
    batch1, batch2, batch3 = random_batchref(1), random_batchref(2), random_batchref(3)
    add_stock([
        (batch1, sku, 100, '2011-01-02'),
        (batch2, sku, 100, '2011-01-01'),
        (batch3, othersku, 100, None),
    ])
    data = {'orderid': random_orderid(), 'sku': sku, 'qty': 3}
    url = config.get_api_url()
    r = requests.post(f'{url}/allocate', json=data)
    assert r.status_code == 201
    assert r.json()['batchref'] == batch2


@pytest.mark.usefixtures('restart_api')
def test_unhappy_path_returns_400_and_error_message():
    unknown_sku, orderid = random_sku(), random_orderid()
    data = {'orderid': orderid, 'sku': unknown_sku, 'qty': 20}
    url = config.get_api_url()
    r = requests.post(f'{url}/allocate', json=data)
    assert r.status_code == 400
    assert r.json()['message'] == f'Invalid sku {unknown_sku}'
----
====

We've successfully split our tests into two broad categories: tests about web
stuff, which we implement end-to-end; and tests about orchestration stuff, which
we can test against the service layer in memory.


=== How Is Our Test Pyramid Looking?

Let's see what this move to using a Service Layer, with its own service-layer tests,
does to our test pyramid:

[[test_pyramid]]
.Counting different types of test
====
[source,sh]
[role="skip"]
----
$ grep -c test_ test_*.py
test_allocate.py:4
test_batches.py:8
test_services.py:3

test_orm.py:6
test_repository.py:2

test_api.py:2
----
====

//NICE-TO-HAVE: test listing this too?

Not bad!  15 unit tests, 8 integration tests, and just 2 end-to-end tests.  That's
a healthy-looking test pyramid.



=== Should Domain Layer Tests Move to the Service Layer?

We could take this a step further. Since we can test our software against
the service layer, we don't really need tests for the domain model any more.
Instead, we could rewrite all of the domain-level tests from Chapter 1 in
terms of the service layer.


.Rewriting a domain test at the service layer (test_services.py)
====
[source,python]
[role="skip"]
----
# domain-layer test:
def test_prefers_current_stock_batches_to_shipments():
    in_stock_batch = Batch("in-stock-batch", "RETRO-CLOCK", 100, eta=None)
    shipment_batch = Batch("shipment-batch", "RETRO-CLOCK", 100, eta=tomorrow)
    line = OrderLine("oref", "RETRO-CLOCK", 10)

    allocate(line, [in_stock_batch, shipment_batch])

    assert in_stock_batch.available_quantity == 90
    assert shipment_batch.available_quantity == 100


# service-layer test:
def test_prefers_warehouse_batches_to_shipments():
    in_stock_batch = Batch("in-stock-batch", "RETRO-CLOCK", 100, eta=None)
    shipment_batch = Batch("shipment-batch", "RETRO-CLOCK", 100, eta=tomorrow)
    repo = FakeRepository([warehouse_batch, shipment_batch])
    session = FakeSession()

    line = OrderLine('oref', "RETRO-CLOCK", 10)

    services.allocate(line, repo, session)

    assert warehouse_batch.available_quantity == 90
----
====

Why would we want to do that?

Tests are supposed to help us change our system fearlessly, but very often
we see teams writing too many tests against their domain model. This causes
problems when they come to change their codebase, and find that they need to
update tens or even hundreds of unit tests.

// TODO (EJ3)  I think this is one of those things that borders on a war of
// religion.  Might want to have some sidebar on BDD, and the perils of test
// coverage metrics.

This makes sense if you stop to think about the purpose of automated tests. We
use tests to enforce that some property of the system doesn't change while we're
working. We use tests to check that the API continues to return 200, that the
database session continues to commit, and that orders are still being allocated.

If we accidentally change one of those behaviors, our tests will break. The
flip side, though, is that if we want to change the design of our code, any
tests relying directly on that code will also fail.

NOTE: Every line of code that we put in a test is like a blob of glue, holding
    the system in a particular shape.

As we get further into the book, we'll see how the service layer forms an API
for our system that we can drive in multiple ways. Testing against this API
reduces the amount of code that we need to change when we refactor our domain
model. If we restrict ourselves to only testing against the service layer,
we won't have any tests that directly interact with "private" methods or
attributes on our model objects, which leaves us more free to refactor them.


[[kinds_of_tests]]
=== On Deciding What Kind of Tests to Write

You might be asking yourself "should I rewrite all my unit tests, then? Is it
wrong to write tests against the domain model?" To answer the question, it's
important to understand the trade-off between coupling and design feedback (see
<<test_spectrum_diagram>>.)

[[test_spectrum_diagram]]
.The test spectrum
image::images/test_spectrum_diagram.png[]
[role="image-source"]
----
[ditaa, test_spectrum_diagram]
| Low feedback                                                   High feedback |
| Low barrier to change                                 High barrier to change |
| High system coverage                                        Focused coverage |
|                                                                              |
| <---------                                                       ----------> |
| API tests                  service-layer tests                  domain tests |
----




Extreme programming (XP) exhorts us to "listen to the code." When we're writing
tests, we might find that the code is hard to use, or notice a code smell. This
is a trigger for us to refactor, and reconsider our design.

We only get that feedback, though, when we're working closely with the target
code. A test for the HTTP API tells us nothing about the fine-grained design of
our objects, because it sits at a much higher level of abstraction.

On the other hand, we can rewrite our entire application and, so long as we
don't change the URLs or request formats, our http tests will continue to pass.
This gives us confidence that large-scale changes, like changing the DB schema,
haven't broken our code.

At the other end of the spectrum, the tests we wrote in chapter 1 helped us to
flesh out our understanding of the objects we need. The tests guided us to a
design that makes sense and reads in the domain language. When our tests read
in the domain language, we feel comfortable that our code matches our intuition
about the problem we're trying to solve.

Because the tests are written in the domain language, they act as living
documentation for our model. A new team member can read these tests to quickly
understand how the system works, and how the core concepts interrelate.

We often "sketch" new behaviors by writing tests at this level to see how the
code might look.

When we want to improve the design of the code, though, we will need to replace
or delete these tests, because they are tightly coupled to a particular
implementation.

// TODO: (EJ3) an example that is overmocked would be good here if you decide to
// add one. Ch12 already has one that could be expanded.

// TODO (SG) - maybe we could do with a/some concrete examples here?  Eg an
// example where a unit test would break but a service-layer test wouldn't?
// and maybe make the analogy of "you should only write tests against public
// methods of your classes, and the service layer is just another more-public
// layer


==== Low and High Gear

Most of the time, when we are adding a new feature, or fixing a bug, we don't
need to make extensive changes to the domain model. In these cases, we prefer
to write tests against services because of the lower coupling and higher coverage.

For example, when writing an `add_stock` function, or a `cancel_order` feature,
we can work more quickly and with less coupling by writing tests against the
service layer.

When starting out a new project, or when we hit a particularly gnarly problem,
we will drop back down to writing tests against the domain model, so that we
get better feedback and executable documentation of our intent.

The metaphor we use is that of shifting gears. When starting a journey, the
bicycle needs to be in a low gear so that it can overcome inertia. Once we're off
and running, we can go faster and more efficiently by changing into a high gear;
but if we suddenly encounter a steep hill, or we're forced to slow down by a
hazard, we again drop down to a low gear until we can pick up speed again.



[[types_of_test_rules_of_thumb]]
.Different Types of Test: Rules of Thumb
******************************************************************************

* Write one end-to-end test per featurefootnote:[What about happy path and
  unhappy path? We say, error-handling is a feature, so yes you need one E2E
  test for error handling, but probably not one unhappy-path test per feature]
  to demonstrate that the feature exists and is working. This might be written
  against an HTTP api. These tests cover an entire feature at a time.

* Write the bulk of the tests for your system against the service layer. This
  offers a good trade-off between coverage, run-time, and efficiency. These
  tests tend to cover one code path of a feature and use fakes for IO.
footnote:[
A valid concern about writing tests at a higher level is that it can lead to
combinatorial explosion, for more complex use cases.  In these cases dropping
down to lower-level unit tests of the various collaborating domain objects
can be useful.  But see also <<chapter_07_events_and_message_bus>> and
<<fake_message_bus>>]

* Maintain a small core of tests written against your domain model. These tests
  have highly-focused coverage, and are more brittle, but have the highest
  feedback. Don't be afraid to delete these tests if the functionality is
  later covered by tests at the service layer.

******************************************************************************


[[primitive_obsession]]
=== Fully Decoupling the Service Layer Tests From the Domain

We still have some direct dependencies on the domain in our service-layer
tests, because we use domain objects to set up our test data and to invoke
our service-layer functions.

To have a service layer that's fully decoupled from the domain, we need to
rewrite its API to work in terms of primitives.

Our service layer currently takes an `OrderLine` domain object:

[[service_domain]]
.Before: allocate takes a domain object (services.py)
====
[source,python]
[role="skip"]
----
def allocate(line: OrderLine, repo: AbstractRepository, session) -> str:
----
====

How would it look if its parameters were all primitive types?

[[service_takes_primitives]]
.After: allocate takes strings and ints (services.py)
====
[source,python]
----
def allocate(
        orderid: str, sku: str, qty: int, repo: AbstractRepository, session
) -> str:
----
====


We rewrite the tests in those terms as well:


[[tests_call_with_primitives]]
.Tests now use primitives in function call (test_services.py)
====
[source,python]
[role="non-head"]
----
def test_returns_allocation():
    batch = model.Batch("batch1", "COMPLICATED-LAMP", 100, eta=None)
    repo = FakeRepository([batch])

    result = services.allocate("o1", "COMPLICATED-LAMP", 10, repo, FakeSession())
    assert result == "batch1"
----
====

But our tests still depend on the domain, because we still manually instantiate
`Batch` objects.  So if, one day, we decide to massively refactor how our Batch
model works, we'll have to change a bunch of tests.


==== Mitigation: Keep All Domain Dependencies in Fixture Functions

We could at least abstract that out to a helper function or a fixture
in our tests.  Here's one way you could do that, adding a factory
function on `FakeRepository`:


[[services_factory_function]]
.Factory functions for fixtures are one possibility (test_services.py)
====
[source,python]
[role="skip"]
----
class FakeRepository(set):

    @staticmethod
    def for_batch(ref, sku, qty, eta=None):
        return FakeRepository([
            model.Batch(ref, sku, qty, eta),
        ])

    ...


def test_returns_allocation():
    repo = FakeRepository.for_batch("batch1", "COMPLICATED-LAMP", 100, eta=None)
    result = services.allocate("o1", "COMPLICATED-LAMP", 10, repo, FakeSession())
    assert result == "batch1"
----
====


At least that would move all of our tests' dependencies on the domain
into one place.


==== Adding a Missing Service

We could go one step further though.  If we had a service to add stock,
then we could use that, and make our service-layer tests fully expressed
in terms of the service layer's official use cases, removing all dependencies
on the domain:


[[test_add_batch]]
.Test for new add_batch service (test_services.py)
====
[source,python]
----
def test_add_batch():
    repo, session = FakeRepository([]), FakeSession()
    services.add_batch("b1", "CRUNCHY-ARMCHAIR", 100, None, repo, session)
    assert repo.get("b1") is not None
    assert session.committed
----
====


TIP: In general, if you find yourself needing to do domain-layer stuff directly
    in your service-layer tests, it may be an indication that your service
    layer is incomplete.


And the implementation is just two lines

[[add_batch_service]]
.A new service for add_batch (services.py)
====
[source,python]
----
def add_batch(
        ref: str, sku: str, qty: int, eta: Optional[date],
        repo: AbstractRepository, session,
):
    repo.add(model.Batch(ref, sku, qty, eta))
    session.commit()


def allocate(
        orderid: str, sku: str, qty: int, repo: AbstractRepository, session
) -> str:
    ...
----
====

NOTE: Should you write a new service just because it would help remove
    dependencies from your tests?  Probably not.  But in this case, we
    almost definitely would need an `add_batch` service one day anyway.

That now allows us to rewrite _all_ of our service-layer tests purely
in terms of the services themselves, using only primitives, and without
any dependencies on the model.


[[services_tests_all_services]]
.Services tests now only use services (test_services.py)
====
[source,python]
----
def test_allocate_returns_allocation():
    repo, session = FakeRepository([]), FakeSession()
    services.add_batch("batch1", "COMPLICATED-LAMP", 100, None, repo, session)
    result = services.allocate("o1", "COMPLICATED-LAMP", 10, repo, session)
    assert result == "batch1"


def test_allocate_errors_for_invalid_sku():
    repo, session = FakeRepository([]), FakeSession()
    services.add_batch("b1", "AREALSKU", 100, None, repo, session)

    with pytest.raises(services.InvalidSku, match="Invalid sku NONEXISTENTSKU"):
        services.allocate("o1", "NONEXISTENTSKU", 10, repo, FakeSession())
----
====


This is a really nice place to be in.  Our service-layer tests only depend on
the services layer itself, leaving us completely free to refactor the model as
we see fit.

=== Carrying the Improvement Through to the E2E Tests

In the same way that adding `add_batch` helped decouple our services-layer
tests from the model, adding an API endpoint to add a batch would remove
the need for the ugly `add_stock` fixture, and our E2E tests can be free
of those hardcoded SQL queries and the direct dependency on the database.

The service function means adding the endpoint is very easy, just a little
json-wrangling and a single function call:


[[api_for_add_batch]]
.API for adding a batch (flask_app.py)
====
[source,python]
----
@app.route("/add_batch", methods=['POST'])
def add_batch():
    session = get_session()
    repo = repository.SqlAlchemyRepository(session)
    eta = request.json['eta']
    if eta is not None:
        eta = datetime.fromisoformat(eta).date()
    services.add_batch(
        request.json['ref'], request.json['sku'], request.json['qty'], eta,
        repo, session
    )
    return 'OK', 201
----
====

NOTE: Are you thinking to yourself `POST` to `/add_batch`?? That's not
    very RESTful!  You're quite right.  We're being happily sloppy, but
    if you'd like to make it all more RESTey, maybe a POST to `/batches`,
    then knock yourself out!  Because Flask is a thin adapter, it'll be
    easy.  See the next sidebar.

And our hardcoded SQL queries from _conftest.py_ get replaced with some
API calls, meaning the API tests have no dependencies other than the API,
which is also very nice:

[[api_tests_with_no_sql]]
.API tests can now add their own batches (test_api.py)
====
[source,python]
----
def post_to_add_batch(ref, sku, qty, eta):
    url = config.get_api_url()
    r = requests.post(
        f'{url}/add_batch',
        json={'ref': ref, 'sku': sku, 'qty': qty, 'eta': eta}
    )
    assert r.status_code == 201


@pytest.mark.usefixtures('postgres_db')
@pytest.mark.usefixtures('restart_api')
def test_happy_path_returns_201_and_allocated_batch():
    sku, othersku = random_sku(), random_sku('other')
    batch1, batch2, batch3 = random_batchref(1), random_batchref(2), random_batchref(3)
    post_to_add_batch(batch1, sku, 100, '2011-01-02')
    post_to_add_batch(batch2, sku, 100, '2011-01-01')
    post_to_add_batch(batch3, othersku, 100, None)
    data = {'orderid': random_orderid(), 'sku': sku, 'qty': 3}
    url = config.get_api_url()
    r = requests.post(f'{url}/allocate', json=data)
    assert r.status_code == 201
    assert r.json()['batchref'] == batch2
----
====


.Exercise for the Reader
******************************************************************************
We've now got services for `add_batch` and `allocate`, why not build out
a service for `deallocate`?  We've added an E2E test and a few stub
service-layer tests for you to get started here:

https://github.com/cosmicpython/code/tree/chapter_04_service_layer_exercise

If that's not enough, continue into the E2E tests and _flask_app.py_, and
refactor the Flask adapter to be more RESTful.  Notice how doing so doesn't
require any change to our service layer or domain layer!

TIP: If you decide you want to build a read-only endpoint for retrieving allocation
    info, just do the simplest thing that can possibly work (TM), which is
    `repo.get()` right in the Flask handler.  We'll talk more about reads vs
    writes in <<chapter_11_cqrs>>.

******************************************************************************

[[why_is_everything_a_service]]
=== Why Is Everything Called A Service?

Some of you are probably scratching your heads at this point trying to figure
out exactly what is the difference between a Domain Service and a Service Layer.

We're sorry, we didn't choose the names, or we'd have much cooler and friendlier
ways to talk about this stuff.

We're using two things called a "service" in this chapter. The first is an
Application Service (our service layer). Its job is to handle requests from the
outside world, and to _orchestrate_ an operation. What we mean is that the
service layer _drives_ the application, by following a bunch of simple steps:

* Get some data from the database
* Update the domain model
* Persist any changes

This is the kind of boring work that has to happen for every operation in your
system, and keeping it separate from business logic helps to keep things tidy.

The second type of service is a Domain Service. This is the name for a piece of
logic that belongs in the domain model but doesn't sit naturally inside a
stateful entity or value object. For example, if you were building a shopping
cart application, you might choose to build taxation rules as a Domain Service.
Calculating tax is a separate job from updating the cart, and it's an important
part of the model, but it doesn't seem to right to have a persisted entity for
the job. Instead a stateless TaxCalculator class, or a calculate_tax function
can do the job.


=== Putting things in folders to see where everything belongs

[[nested_folder_tree]]
.Some subfolders
====
[source,text]
[role="skip"]
----
.
├── config.py
├── domain  #<1>
│   ├── __init__.py
│   └── model.py
├── service_layer  #<2>
│   ├── __init__.py
│   └── services.py
├── adapters  #<3>
│   ├── __init__.py
│   ├── orm.py
│   └── repository.py
├── entrypoints  <4>
│   ├── __init__.py
│   └── flask_app.py
└── tests
    ├── __init__.py
    ├── conftest.py
    ├── unit
    │   ├── test_allocate.py
    │   ├── test_batches.py
    │   └── test_services.py
    ├── integration
    │   ├── test_orm.py
    │   └── test_repository.py
    └── e2e
        └── test_api.py

----
====

<1> Let's have a folder for our domain model.  Currently that's just one file,
    but for a more complex application you might have one file per class, you
    might have some helper parent classes for `Entity`, `ValueObject` and
    `Aggregate`, you might add an __exceptions.py__ for domain-layer exceptions,
    and as we'll see in Part 2, __commands.py__ and __events.py__.

<2> We'll distinguish the service layer.  Currently that's just one file
    called _services.py_ for our service-layer functions.  You could
    add service-layer exceptions in here, and as we'll see in the next
    chapter, we'll add __unit_of_work.py__

<3> Adapters is a nod to the Ports and Adapters terminology.  This will fill
    up with any other abstractions around external I/O, eg a __redis_client.py__.
    Strictly speaking you would call these _secondary_ adapters or _driven_
    adapters, or sometimes _inward-facing_ adapters.

<4> Entrypoints are the places we drive our application from.  In the
    official Ports & Adapters terminology, these are adapters too, and are
    referred to as _primary_,  _driving_ or _outward-facing_ adapters.

What about ports?  As you may remember, they are the abstract interfaces which the
adapters implement. If you really wanted to go to town, you could split out the
abstract repository (which is the Port) into its own file, along with any other
ABCs for other adapters, and put them in a folder called __ports__footnote:[Our
tech reviewer David Seddon would particularly enjoy that, because it would make
our module dependency graph really neat, illustrating the "depend on abstractions"
concept.].

[[abstract_repo_own_file]]
.Some subfolders
====
[source,text]
[role="skip"]
----
.
├── domain
│   ├── __init__.py
│   └── model.py
├── adapters
│   ├── __init__.py
│   ├── orm.py
│   └── sqlalchemy_repository.py
├── ports
│   ├── abstract_repository.py
...
----
====

The primary/upstream ports are harder to define at this stage, because
the port is just "the service-layer function and its arguments". In
later chapter, we build Command and Event classes, and you can call
those ports in theory, but putting them into a _ports_ subfolder instead
of inside the domain folder would feel weird to us.

// TODO - this whole ports discussion feels a bit long-winded, maybe cut?
// (EJ3) - I think this section could be shortened up a bit.  Re: the 'ports'
//         package/folder, I get the value for pedagogical purposes here, but
//         in general I'm ambivalent because:
//         * in practice, someone may cargo-cult the 'adapters' + 'ports' structure naively,
//           and end up sticking anything that is a port or adapter here. And then
//           you will end up coincidental coupling.
//         * any package structure is subjective and fail you eventually, which is the
//           how you get crosscuts.


=== Wrap-Up


Adding the service layer has really bought us quite a lot:

* Our Flask API endpoints become very thin and easy to write:  their
  only responsibility is doing "web stuff," things like parsing JSON
  and producing the right HTTP codes for happy or unhappy cases.

* We've defined a clear API for our domain, a set of use cases or
  entrypoints that can be used by any adapter without needing to know anything
  about our domain model classes--whether that's an API, a CLI (see
  <<appendix_csvs>>), or the tests! They're an adapter for our domain too.

* We can write tests in "high gear" using the service layer, leaving us
  free to refactor the domain model in any way we see fit.  As long as
  we can still deliver the same use cases, we can experiment with new
  designs without needing to rewrite a load of tests.

* And our "test pyramid" is looking good--the bulk of our tests
  are fast unit tests, with just the bare minimum of E2E and integration
  tests.


==== The DIP in Action

<<service_layer_diagram_abstract_dependencies>> shows the
dependencies of our service layer: the Domain Model,
and the `AbstractRepository` (the port, in ports & adapters terminology).

[[service_layer_diagram_abstract_dependencies]]
.Abstract dependencies of the service layer
image::images/service_layer_diagram_abstract_dependencies.png[]
[role="image-source"]
----
[ditaa, service_layer_diagram_abstract_dependencies]
        +-----------------------------+
        |         Service Layer       |
        +-----------------------------+
           |                   |
           |                   | depends on abstraction
           V                   V
+------------------+     +--------------------+
|   Domain Model   |     | AbstractRepository |
|                  |     |       (Port)       |
+------------------+     +--------------------+
----


When we run the tests, <<service_layer_diagram_test_dependencies>> shows
how we implement the abstract dependencies using `FakeRepository` (the
adapter):

[[service_layer_diagram_test_dependencies]]
.Tests provide an implementation of the abstract dependency
image::images/service_layer_diagram_test_dependencies.png[]
[role="image-source"]
----
[ditaa, service_layer_diagram_test_dependencies]
        +-----------------------------+
        |           Tests             |-------------\
        +-----------------------------+             |
                       |                            |
                       V                            |
        +-----------------------------+             |
        |         Service Layer       |    provides |
        +-----------------------------+             |
           |                     |                  |
           V                     V                  |
+------------------+     +--------------------+     |
|   Domain Model   |     | AbstractRepository |     |
+------------------+     +--------------------+     |
                                    ^               |
                         implements |               |
                                    |               |
                         +----------------------+   |
                         |    FakeRepository    |<--/
                         |      (in-memory)     |
                         +----------------------+
----

And when we actually run our app, we swap in the "real" dependency,
<<service_layer_diagram_runtime_dependencies>>:

[[service_layer_diagram_runtime_dependencies]]
.Dependencies at runtime
image::images/service_layer_diagram_runtime_dependencies.png[]
[role="image-source"]
----
[ditaa, service_layer_diagram_runtime_dependencies]
       +--------------------------------+
       | Flask API (Presentation layer) |-----------\
       +--------------------------------+           |
                       |                            |
                       V                            |
        +-----------------------------+             |
        |         Service Layer       |             |
        +-----------------------------+             |
           |                     |                  |
           V                     V                  |
+------------------+     +--------------------+     |
|   Domain Model   |     | AbstractRepository |     |
+------------------+     +--------------------+     |
              ^                     ^               |
              |                     |               |
       gets   |          +----------------------+   |
       model  |          | SqlAlchemyRepository |<--/
   definitions|          +----------------------+
       from   |                | uses
              |                V
           +-----------------------+
           |          ORM          |
           | (another abstraction) |
           +-----------------------+
                       |
                       | talks to
                       V
           +------------------------+
           |       Database         |
           +------------------------+
----



//TODO (DS): Good wrap up. I'd really like to see a table or something that
//sums up what belongs in each layer so far.

Wonderful. But there's still a bit of awkwardness we'd like to get rid of. The
service layer is tightly coupled to a `session` object.  In the next chapter,
we'll introduce one more pattern that works closely with Repository and
Service Layer, the Unit of Work pattern, and everything will be absolutely
lovely. You'll see!

But first, as is customary, a pause for <<chapter_04_service_layer_tradeoffs>>,
in which we consider the pros and cons of having a Service Layer at all.


[[chapter_04_service_layer_tradeoffs]]
[options="header"]
.Service Layer: The Tradeoffs
|===
|Pros|Cons
a|
* We've got a single place to capture all the use cases for our application.

* We've placed our clever domain logic behind an API which leaves us free to
  refactor.

* We have cleanly separated "stuff that talks HTTP" from "stuff that talks
  allocation".

* When combined with _Repository Pattern_ and a `FakeRepository`, we've got
  a nice way of writing tests at a higher level than the Domain Layer;
  we can test more of our workflow without needing to go to integration tests.

a|
* If your app is _purely_ a web app, your controllers/view functions can be
  the single place to capture all the use cases.

* It's yet another layer of abstraction.

* Putting too much logic into the service layer can lead to the _Anemic Domain_
  anti pattern. It's better to introduce this layer once you spot orchestration
  logic creeping into your controllers.

* You can get a lot of the benefits that come from having rich domain models
  by simply pushing logic out of your controllers and down to the model layer,
  without needing to add an extra layer in between (aka "fat models, thin
  controllers")
|===
